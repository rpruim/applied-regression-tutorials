[
  {
    "objectID": "r-basics.html#your-mission",
    "href": "r-basics.html#your-mission",
    "title": "R Basics",
    "section": "Your Mission",
    "text": "Your Mission\nThe purpose of this tutorial is to help you start to get familiar with the way R works, and some basic R commands. \nThis tutorial environment uses webr, which lets you read some helpful information, then immediately practice writing and running your own R code, all in your web browser.\nHere’s hoping it provides a nice, gentle introduction in a controlled environment!",
    "crumbs": [
      "Home",
      "R Basics",
      "R Basics"
    ]
  },
  {
    "objectID": "r-basics.html#communicating-with-r",
    "href": "r-basics.html#communicating-with-r",
    "title": "R Basics",
    "section": "Communicating with R",
    "text": "Communicating with R\nYou will do most of your work in R with code or commands. Instead of pointing and clicking, you will type one or more lines of code, which R will execute (doing the work you have asked it to do).\nThen, R will return the results of whatever operation you asked it to do. That might be a number, or a table, or a plot, depending on what you asked for.\n\n\n\n\n\n\n\n\nSometimes executing code has almost no visible effect (no plot or text output is produced), but instead some object is created and stored in R’s environment for later use.\n\n\n\n\n\n\n\n\nThe value saved in four can be used later.\n\nExercise 1 Add an additional line of code in the previous chunk containing just four\nNow what happens when you run the code chunk?\n\n\nTwo Key Questions\nTo get R (or any software) to do something for you, there are two important questions you must be able to answer. Before continuing, think about what those questions might be.\n\n\nThe Questions\nTo get R (or any software) to do a job for you, there are two important questions you must be able to answer:\n\n1. What do you want the computer to do?\n\n\n2. What must the computer know in order to do that?\n\n\n\nProviding R with the information it needs\nR functions provide R with the answer to the first question: what do you want the computer to do?\nMost functions in R have short, but descriptive names that describe what they do. For example, R has some functions to do basic mathematical operations: the function sqrt() computes the square root of a number, and the function round() rounds a number (by default, it rounds to the nearest integer).\nBut just giving R a function is not enough: you also need to answer the second question (what information does R need to do the job?). For example, if you want to use the function round(), you also need to provide R with the number you want to round!\nWe will provide answers to our two questions by filling in the boxes of a basic template:\n\n\nfunction (  information1  ,  information2  , …)\n\n\n \n(The ... indicates that there may be some additional input arguments (input information we could provide to R) we could add eventually. Some functions need only one input, but if a function takes more than one argument, they are separated by commas. They have names, and if named (like: function(input_name = value, input2_name = 'value')) they can be in any order.\n\n\nUsing simple functions\nLet’s practice what you just learned, trying out the mathematical sqrt() and round() functions.\n\nExercise 2 Edit the code below to compute the square root of 64:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHint 1\n\n\n\n\n\nConsider using the sqrt() function:\nsqrt(___)\n\n\n\n\n\n\n\n\n\n\n\nNoteHint 2\n\n\n\n\n\nThe input information that sqrt() needs to make your calculation is the number you want the square root of: 64.\n\n\n\n\n\n\n\n\n\n\n\nTipSolution:\n\n\n\n\n\nsqrt(64)\n\n\n\n\n\n\nExercise 3 Now try computing the square root of 44, and then rounding it to the nearest integer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHint 1\n\n\n\n\n\nYou’ll need to use two functions this time:\nThe sqrt() function, and then the round() function.\nsqrt(___)\nround(___)\n\n\n\n\n\n\n\n\n\n\n\nNoteHint 2\n\n\n\n\n\nThe input information that sqrt() needs to make your calculation is the number you want the square root of: 44. Run that code first, to get the input you will need for round()…\nsqrt(44)\nround(___)\n\n\n\n\n\n\n\n\n\n\n\nTipSolution:\n\n\n\n\n\nsqrt(44)\nround(6.63325)\nCan you do it all in one go? Well…yes!\nround(sqrt(44))\nThere’s also an easier-to-read way to do that, using a pipe operator |&gt;. It takes the output of one operation and passes it as input to the next. You can read it as |&gt; = “and then…” so we could do:\nsqrt(44) |&gt;\n  round()\n\nTake the square root of 44, and then\nround the result.\n\n(More on pipes later!)\n\n\n\n\n\n\n\nStoring information in R: variables\nIn the last section, you computed the square root of 44 and then rounded it, perhaps like this:\n\nsqrt(44)\n\n[1] 6.63325\n\nround(6.63325)\n\n[1] 7\n\n\nBut to do that, you probably had to first find the root, make a note of the result, and then provide that number to the round function. What a pain!\nA very useful option, if you have value (or a variable, dataset, or other R object) that you will want to use later on, is to store it as a named object in R. In the previous example, you might want to store the square root of 44 in a variable called my_root; then you can provide my_root to the round() function without checking the result of the sqrt() calculation first:\n\nmy_root &lt;- sqrt(44)\nround(my_root)\n\n[1] 7\n\n\nNotice that to assign a name to the results of some R code, you use the symbol &lt;-. You can think of it as an assignment arrow – it points from a value or item toward a name and assigns the name to the thing.\n\nExercise 4 Try editing the code to change the name of the variable from my_root to something else, then run your new code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nMake sure you change the name my_root in both places.\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nyour_new_name &lt;- sqrt(44)\nround(your_new_name)\n\n\n\n\n\n\n\nWhat if I have a list of numbers to store?\nSometime you might want to create a variable that contains more than one number. You can use the function c() to concatenate a list of numbers:\n\nmy_fave_numbers &lt;- c(4, 44, 16)\nmy_fave_numbers\n\n[1]  4 44 16\n\n\n(First we stored the list of numbers, calling it my_fave_numbers; then we printed the results to the screen by simply typing the variable name my_fave_numbers).\n\nExercise 5 Try making a list of your three favorite numbers, then using the function sum to add them all up:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nFirst use c() to concatenate your chosen numbers (separated by commas).\nDon’t forget to use &lt;- to assign your list of numbers a name!\nThen, use sum() to add them up.\nmy_numbers &lt;- c(___,___,___)\nsum(___)\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThis is just one possible solution.\nmy_numbers &lt;- c(4, 16, 44)\nsum(my_numbers)\nNotice you could also nest sum() and c(), or use the pipe operator |&gt; to calculate the numeric answer, but then you would not have the object my_numbers available for later use…\nsum(c(4, 16, 44))\n# or \nc(4, 16, 44) |&gt;\n  sum()\n\n\n\n\n\n\n\nWhat about data that are not numeric?\nR can work with categorical data as well as numeric data. For example, we could create a list of words and store it as a variable if we wanted (feel free to try changing the words if you want):\n\n\n\n\n\n\n\n\n\n\nWhat if I have a LOT more data to store?\nc() works great for creating small lists of just a few values, but it is not a good way to enter or store large data tables - there is lots of potential for user error. In this course, you will usually be given a dataset already in electronic form; if you need to create one, you would turn to spreadsheet or database software. Either way you read the existing data file into R directly.\nIn R, these larger datasets are stored as objects called data.frames. The next sections will get you started using them.",
    "crumbs": [
      "Home",
      "R Basics",
      "R Basics"
    ]
  },
  {
    "objectID": "r-basics.html#how-should-data-tables-be-organized-for-statistical-analysis",
    "href": "r-basics.html#how-should-data-tables-be-organized-for-statistical-analysis",
    "title": "R Basics",
    "section": "How should data tables be organized for statistical analysis?",
    "text": "How should data tables be organized for statistical analysis?\nA comprehensive guide to good practices for formatting data tables is available at http://kbroman.org/dataorg/.\nA few key points to keep in mind:\n\nThis data table is for the computer to read, not for humans! So eliminate formatting designed to make it “pretty” (color coding, shading, fonts…)\nUse short, simple variable names that do not contain any spaces or special characters (like ?, $, %, -, etc.)\nOrganize the table so there is one column for every variable, and one row for every observation (person/place/thing for which data were collected).\nUse informative variable values rather than arbitrary numeric codes. For example, a variable Color should have values ‘red’, ‘white’, and ‘blue’ rather than 1, 2, and 3.\n\nYou will have chances to practice making your own data files and importing them into R outside this tutorial.",
    "crumbs": [
      "Home",
      "R Basics",
      "R Basics"
    ]
  },
  {
    "objectID": "r-basics.html#using-built-in-datasets-in-r",
    "href": "r-basics.html#using-built-in-datasets-in-r",
    "title": "R Basics",
    "section": "Using built-in datasets in R",
    "text": "Using built-in datasets in R\nR has a number of built-in datasets that are accessible to you as soon as you start RStudio.\nIn addition to the datasets that are included with base R, there are add-on packages for R that contain additional software tools and sometimes datasets.\nTo use datasets contained in a package, you have to load the package by running the command:\n\nlibrary(packagename) \n\n\nExample of loading a package\nFor example, we will practice looking at a dataset from the package mosaic.\nBefore we can access the data, we have to load the package. The code might look like this:\n\nlibrary(mosaic)\n\n(Nothing obvious will happen when you run this code…it basically just gives R permission to access the package, so there is often no output visible.)\n\n\nViewing a dataset\nThe mosaic package includes a dataset called HELPrct.\nIf you just run the dataset name (HELPrct) as a command, R will print some (or all - egad!) of the dataset out to the screen! (So don’t…)\nBut…how can we extract selected, useful information about a dataset?\n\n\nGathering information about a dataset\nThere are a few functions that make it easier to take a quick look at a dataset:\n\nhead() prints out the first few rows of the dataset.\nnames() prints out the names of the variables (columns) in the dataset\ndplyr::glimpse() (function glimpse() from package dplyr) gives an short list-like overview of the dataset\nskimr::skim() (function skim() from the package skimr) prints out more detailed graphical summary information about a dataset\nnrow() reports the number of rows (observations or cases) in the dataset\nncol() reports the number of columns (variables) in the dataset\n\n\nExercise 6 Try applying each of these functions to the HELPrct data and see what the output looks like each time:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe input for each of the functions is the name of the dataset: HELPrct.\nhead(HELPrct)\nnames(HELPrct)\nnrow(HELPrct)\nncol(HELPrct)\nskimr::skim(HELPrct)\ndplyr::glimpse(HELPrct)\nIn this case, the point is usually to view the information on-screen, not to store it for later use, so we have not used &lt;- at all to store any output for later use or reference.\n\n\n\n\n\nGetting more help\nYou can get help related to R function, and built-in R datasets, using a special function: ?. Just type ? followed by the name of the function or dataset you want help on:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor example, if you want to know about the function nrow():\n?nrow",
    "crumbs": [
      "Home",
      "R Basics",
      "R Basics"
    ]
  },
  {
    "objectID": "r-basics.html#reading-in-data-from-a-file",
    "href": "r-basics.html#reading-in-data-from-a-file",
    "title": "R Basics",
    "section": "Reading in data from a file",
    "text": "Reading in data from a file\nFor this class, you will often be asked to analyze data that is stored in files that are available online - usually in csv format. It’s simple to read them into R. For example, we can read in the file MI_lead.csv, which is stored at https://sldr.netlify.app/data/MI_lead.csv using the function readr::read_csv() (from package readr or super-package tidyverse):\n\n# Note: You don't have to (and can't) run this code here in the tutorial,\n# but the data set has already been pre-loaded for you.\n\nlibrary(readr) # the readr package contains the read_csv() function\nMI_lead &lt;- \n  read_csv(file = 'https://sldr.netlify.app/data/MI_lead.csv')\n\n\n\n\n\n\n\nWarningLoading data from netlify doesn’t work in this tutorial.\n\n\n\nInstead, I’ve put the data into a data/ folder that is available in this tutorial. So we can read it in like this:\n\n\n\n\n\n\n\n\nYou could do the same thing in your quarto file if you first download the data set and put it into a data/ folder. But in that situation, it is just as convenient to read it from the internet.\n\n\n\nExercise 7 Try changing MI_lead.csv to one of the other files listed by dir('data').\n\n\nThe most common mistakes\nThe code below contains several of the most common mistakes students make when they try to read in a data file.\n\nExercise 8 See if you can find and correct all the errors in the code below.\nThe code below - if corrected - would  run without an error and read in some baseball statistics from the file https://stacyderuiter.github.io/teachingdata/data-raw/baseball.csv.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHints\n\n\n\n\n\nThink about:\n\nIs the filename or URL spelled correctly, with no typos and correct capitalization?\nIs the filename or URL in quotation marks (either ” or ’ work equally)?\nIs the URL complete (including the file extension “.csv” at the end and “https://” or “http://” at the beginning)?\nWas &lt;- used to assign a name to the dataset once read in? (Otherwise it will just be uselessly printed to the screen and not available for later use!)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nbaseball_data &lt;- \n  read_csv(\n    'http://stacyderuiter.github.io/teachingdata/data-raw/baseball.csv'\n  )\n\n\n\n\n\n\n\n\n\n\n\nNoteWarning you can ignore\n\n\n\nWhen you load this data set (after correcting the problems in the code, you will see the message\n! curl package not installed, falling back to using url()\nThat’s just letting you know that in the tutorial environment curl – the preferred tool for downloading files from the internet – is not available, so another method is being used instead. You can safely ignore this warning.\n\n\n\n\nWhat about local files?\nThe same function, read_csv(), can be used to read in a local file. You just need to change the input to read_csv() – instead of a URL, you provide a path and filename (in quotes). For example, the input file = 'https://sldr.netlify.app/data/MI_lead.csv' might become file = 'C:\\\\Data\\\\MI_lead.csv'.\nWe won’t do an example in this tutorial because it’s not straightforward to work with local files within a tutorial environment, but you can practice it once you are working independently in RStudio.\nIf you are working on the server r.stem.calvin.edu, you will have to upload files to your cloud space on the server before you can read them in (RStudio on the server cannot access files on your computer’s hard drive). Look in the “Files” tab on the lower right, and then click “Upload.”\n\n\n\n\n\n\nNoteNamed and positional input arguments\n\n\n\nIn R, arguments to functions can be provided either positionally (in order) or by name. Named arguemnts are matched up first, and then any remaining unnamed arguments are matched in order. Very often the most important arguments are required and at the beginning of the list, so we often omit their names. Later arguments are typically optional and have default values. We can use their names to set just the ones we are interested in changing.\nWith read_csv(), the first argument is file, and we always need to specify it. While you can type file =, we usually won’t do that. We just have to put the file name first in the argument list.\nIn other words, the following are equivalent:\n\nMI_lead &lt;- \n  read_csv(file = 'https://sldr.netlify.app/data/MI_lead.csv')\nMI_lead &lt;- \n  read_csv('https://sldr.netlify.app/data/MI_lead.csv')\n\n\n\n\n\n\n\n\n\nTipBy name or by position?\n\n\n\nBest practice is to use names for all but the first one or two arguments. That makes your code easier to read and avoids mistakes of assigning values to unintended arguments.\n\n\n\n\nRenaming variables in a dataset\nThis is an advanced topic, so don’t worry if it seems complicated; for now, it just nice to realize some of the power R has to clean up and reorganize data.\nWhat if we didn’t like the names of the MI_lead variables? For example, a new user of the dataset might not know that that ELL stands for “elevated lead levels” and that ELL2005 gives the proportion of tested kids who had elevated lead levels in the year 2005.\nIf we wanted to use a clearer (though longer) variable name, we might prefer “prop_elevated_lead_2005” instead of “ELL2005” – more letters to type, but a bit easier to understand for a new user. How can we tell R we want to rename a variable?\nWe use the code:\n\n\n\n\n\n\n\n\nThe code above uses some tools you’ve seen, and some more advanced ones you haven’t seen yet. The symbol |&gt; is called a “pipe” and basically means “and then…” Translated into words, the code above tells R:\n\nMake a dataset called MI_lead by starting with the dataset MI_lead.\nNext, take the results do something more with them (|&gt;) …\nrename() a variable. What I want to rename is the variable ELL2005. Its new name should be prop_elevated_lead_2005.”\n\nSee…you can already start to make sense of even some pretty complicated (and useful) code.\nNote: If you give R several commands, not connected by pipes, it will do the first, then the second, then the third, and so on. R doesn’t need the pipe for permission to continue! Instead, the pipe tells R to take the results from the first command, and use them as the first input for the next command. So the following are equivalent:\nfoo(x, y, z)\nx |&gt; foo(y, z)\nThis is very handy when working with a set of functions that all\n\ntake the same kind of first input,\nreturn that same kind of output.\n\nThese commands can be “chained” together with pipes to do a sequence of operations. A good example of this is the data wrangling functions in the tidyverse packages (like dplyr and tidyr). These all use a data frame as their first argument and return a data frame as their output.\nHere’s the basic template for chaining data transformations:\n\nmodfied_data &lt;-\n  original_data |&gt;\n  data_transformation1(...) |&gt;\n  data_transformation2(...) |&gt;\n  data_transformation3(...) \n\n\n\nCheck out the data\nOK, back to business - simple functions and datasets in R.\n\nExercise 9 It’s your turn to practice now. Use one of the functions you have learned so far to extract some information about the MI_lead dataset.\n\nHow many rows are in the dataset? How many variables (columns)?\nWhat are the variables named, and what are their values like?\n\nRemember, ? won’t work on MI_lead because it’s not a built-in R dataset. Also, the dataset MI_lead is already read in for you, here…so you don’t need to use read_csv().",
    "crumbs": [
      "Home",
      "R Basics",
      "R Basics"
    ]
  },
  {
    "objectID": "r-basics.html#review",
    "href": "r-basics.html#review",
    "title": "R Basics",
    "section": "Review",
    "text": "Review\nWhat have you learned so far? More than you think!\n\nFunctions in R\nYou’ve learned that R code is made up of functions, which are generally named descriptively according to the job they do. Functions have one or more input arguments, which is where you provide R with all the data and information it needs to do the job. The syntax for calling a function uses the template:\n\n\nfunction (  information1  ,  information2  , …)\n\n\n \n\n\nVariables in R\nYou’ve practiced creating variables in R using c(), and saving information (or the results of a computation) using the assignment arrow &lt;-.\n\n\nDatasets in R\nYou’ve considered several different ways to get datasets to work with in R: you can use datasets that are built in to R or R packages, or you can use read_csv() to read in data files stored in .csv format.\n\n\nVocabulary\nYou should now be able to define and work with some R-related terms:\n\ncode or commands that R can execute\nfunction and inputs or arguments\nassignment arrow: &lt;-\npipe = “and then…”: |&gt;\n\n\n\n\n\n\n\nNoteOld pipes\n\n\n\n%&gt;% (in the magrittr package) is an older way of writing a pipe, and it does basically the same thing as |&gt;. There is no reason to use it now, but you may see it in older code examples.",
    "crumbs": [
      "Home",
      "R Basics",
      "R Basics"
    ]
  },
  {
    "objectID": "r-basics.html#congratulations",
    "href": "r-basics.html#congratulations",
    "title": "R Basics",
    "section": "Congratulations!",
    "text": "Congratulations!\nYou just completed your first tutorial on R, and wrote some of your own R code. I knew you could do it…\nWant more help and practice? Consider checking out outside resources from posit: https://posit.cloud/learn/primers",
    "crumbs": [
      "Home",
      "R Basics",
      "R Basics"
    ]
  },
  {
    "objectID": "quarto-results.html",
    "href": "quarto-results.html",
    "title": "R Results in Quarto",
    "section": "",
    "text": "You may also want to include the results of R calculations in the TEXT part of your report.\nLet’s say you compute the mean of some kids’ foot lengths:\n\nmean(~length, data = KidsFeet)\n\n[1] 24.72308\n\n\nYou can use the result in the text part of your file…so you would type:\nThe mean length of the kids’ feet was ` r mean(~length, data=KidsFeet) ` cm.\nTo get:\nThe mean length of the kids’ feet was 24.7230769 cm.\nNote: those accent marks (before the “r” and at the end of the R-code stuff) are not normal single quotes or apostrophes; they are “back-ticks” or “graves” ( ` ), just like those used to help define the start and end of R code chunks in your Quarto file. There should not actually be a space between the ` and the r.\nBut it’s annoying (and sometimes not really practical) to (re)type the entire R command in the text part of your file. An option is to save the quantity you want to refer to as a variable in R:\n\nmean_length &lt;- mean(~length, data = KidsFeet)\n\nThen you can write: The mean foot length of the kids was ` r mean_length` cm.\nTo get: The mean foot length of the kids was 24.7230769 cm.",
    "crumbs": [
      "Home",
      "Quarto",
      "R Results in Text"
    ]
  },
  {
    "objectID": "quarto-results.html#including-results-of-r-calculations-in-your-text",
    "href": "quarto-results.html#including-results-of-r-calculations-in-your-text",
    "title": "R Results in Quarto",
    "section": "",
    "text": "You may also want to include the results of R calculations in the TEXT part of your report.\nLet’s say you compute the mean of some kids’ foot lengths:\n\nmean(~length, data = KidsFeet)\n\n[1] 24.72308\n\n\nYou can use the result in the text part of your file…so you would type:\nThe mean length of the kids’ feet was ` r mean(~length, data=KidsFeet) ` cm.\nTo get:\nThe mean length of the kids’ feet was 24.7230769 cm.\nNote: those accent marks (before the “r” and at the end of the R-code stuff) are not normal single quotes or apostrophes; they are “back-ticks” or “graves” ( ` ), just like those used to help define the start and end of R code chunks in your Quarto file. There should not actually be a space between the ` and the r.\nBut it’s annoying (and sometimes not really practical) to (re)type the entire R command in the text part of your file. An option is to save the quantity you want to refer to as a variable in R:\n\nmean_length &lt;- mean(~length, data = KidsFeet)\n\nThen you can write: The mean foot length of the kids was ` r mean_length` cm.\nTo get: The mean foot length of the kids was 24.7230769 cm.",
    "crumbs": [
      "Home",
      "Quarto",
      "R Results in Text"
    ]
  },
  {
    "objectID": "quarto-results.html#rounding",
    "href": "quarto-results.html#rounding",
    "title": "R Results in Quarto",
    "section": "Rounding",
    "text": "Rounding\nWhat if you want to include a more reasonable number of decimal places? Use : The mean foot length of the kids was ` r round(mean_length, digits = 2)` cm\nand you get: The mean foot length of the kids was 24.72 cm",
    "crumbs": [
      "Home",
      "Quarto",
      "R Results in Text"
    ]
  },
  {
    "objectID": "quarto-results.html#r-results-with-more-than-one-value-inside",
    "href": "quarto-results.html#r-results-with-more-than-one-value-inside",
    "title": "R Results in Quarto",
    "section": "R results with more than one value inside",
    "text": "R results with more than one value inside\nWhat if you have an object with more than one value in it? For example, what if you computes means for both boys and girls? You can use hard brackets ( [ … ] ) to refer to the first, second, etc. entries. For example:\n\ngirlboy.means &lt;- mean(~ length | sex,\n                      data = KidsFeet)\n\nYou type: The girls’ mean foot length was ` r girlboy.means[“G”] `, and the boys’ was ` r girlboy.means[“B”] `\nto get: The girls’ mean foot length was 24.3210526, and the boys’ was 25.105.\nYou can also use numeric indices – for example, ` r girlboy.means[2] ` instead of ` r girlboy.means[“G”] ` to get the girls’ value – but using names when you can is always safer because you don’t have to worry about whether things are stored in the order you think that they are!\nIf you are referring to a data table or other object with multiple rows and columns, you can use the syntax to extract a row, a column, or a specific value of interest. If you leave either or blank, all rows/columns will be included.\nFor example, consider a table showing some data from a survey of intro stat students ( tells whether they have gotten a speeding ticket while driving a car, and tells whether they have texted while driving a car):\n\nstudent_survey &lt;- read.csv('https://sldr.netlify.app/data/IntroStatStudents.csv', \n              na.strings = list('', 'NA'))\ntally(~Ticket | Texted, \n      data = student_survey, \n      format = 'proportion')\n\n               Texted\nTicket                  No        Yes\n  I don't drive 0.03703704 0.00000000\n  No            0.77777778 0.71900826\n  Yes           0.14814815 0.28099174\n  &lt;NA&gt;          0.03703704 0.00000000\n\n\nWhat if we want to print just the first column of data?\n(Note: Don’t count the row and column names when numbering the rows and columns.)\n\ntally(~Ticket | Texted, \n      data = student_survey,\n      format = 'proportion')[,1]\n\nI don't drive            No           Yes          &lt;NA&gt; \n   0.03703704    0.77777778    0.14814815    0.03703704 \n\n\nOr better (and clearer…)\n\ntally(~Ticket | Texted, \n      data = student_survey,\n      format = 'proportion')[, \"No\"]\n\nI don't drive            No           Yes          &lt;NA&gt; \n   0.03703704    0.77777778    0.14814815    0.03703704 \n\n\nWhat about the third row (for people who have gotten a ticket)?\n\ntally(~Ticket | Texted, \n      data = student_survey, \n      format = 'proportion')[\"Yes\",]\n\n       No       Yes \n0.1481481 0.2809917 \n\n\nWhat about the proportion of students with tickets, among those who’ve texted while driving? (Row 3, Column 2 = row “Yes” and column “Yes”)? Let’s first save the table so we don’t have to recompute…\n\ndriver_table &lt;- tally(~Ticket | Texted, \n      data = student_survey, \n      format = 'proportion')\n\nType: The proportion of students who have texted while driving who have gotten a speeding ticket is ` r driver_table[“Yes”,“Yes”] `.\nTo get: The proportion of students who have texted while driving who have gotten a speeding ticket is 0.2809917.\n(Like before, if it’s possible to use names instead of numeric indices, try to do so!)",
    "crumbs": [
      "Home",
      "Quarto",
      "R Results in Text"
    ]
  },
  {
    "objectID": "graphics-design.html",
    "href": "graphics-design.html",
    "title": "Graphics: Design Effective Visualizations",
    "section": "",
    "text": "After this section, you will be able to:\n\nCritique statistical graphics based on design principles.\nRecognize common misleading design choices for data visualizations\nRecognize data visualization that tells a true story, identifying elements that emphasize the main finding and make the figure easy to interpret at a glance",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#module-learning-outcomes",
    "href": "graphics-design.html#module-learning-outcomes",
    "title": "Graphics: Design Effective Visualizations",
    "section": "",
    "text": "After this section, you will be able to:\n\nCritique statistical graphics based on design principles.\nRecognize common misleading design choices for data visualizations\nRecognize data visualization that tells a true story, identifying elements that emphasize the main finding and make the figure easy to interpret at a glance",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#reference-materials",
    "href": "graphics-design.html#reference-materials",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Reference Materials",
    "text": "Reference Materials\n\nBeyond Multiple Linear Regression Ch. 1.5\nEcological Models & Data in R Ch. 2 discusses graphics, but is not recommended as the approach to reading in data, writing R code, and generating graphs in R is very different to that used in this course.\nA comprehensive, and free, supplemental reference is Fundamentals of Data Visualization by Claus Wilke\n\nIt’s suggested that you refer to the above materials as needed after doing this section, with particular focus on the topics you found most challenging.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#inspiration",
    "href": "graphics-design.html#inspiration",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Inspiration",
    "text": "Inspiration\n\nAbove all, show the data.\nE. Tufte, The Visual Display of Quantitative Information\n\nBut…\n\nThe Numbers Don’t Speak for Themselves.\nC. D’Ignazio and L. Klein, Data Feminism\n\nIn visualizing data, we use graphics to gain and communicate an honest understanding of data in context.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#motivation-imagine-first",
    "href": "graphics-design.html#motivation-imagine-first",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Motivation: Imagine First!",
    "text": "Motivation: Imagine First!\nFigures are a crucial tool for exploring your data and communicating what you learn from the data.\nWhether you are doing a quick check to assess basic features of a dataset or creating a key figure for an important presentation, the best practice is to work thoughtfully.\n\nThe I.C.E.E. method:\n\nImagine how you want your graph to look, before you\nCode. Once you have the basic starting point,\nEvaluate your work, and\nElaborate (refine it).\n\nRepeat until the figure is as awesome as it needs to be.\n\n\nNO To Mindless Copy/Paste\nToo many of us fall into the trap of starting to write code (or copy/pasting it!) before pausing to think carefully about the desired outcome, then settling for the first vaguely relevant result (or delighting in the unintended outcome…).\n\n\nYou can do better than mindless copying! Only mindful copy-pasting allowed.\n\n\nThis section provides some advice to get you started. It can also provide inspiration for constructive critique of others’ graphics.\nHere we focus only on the I_EE parts of the process, where you design and assess graphics. Code will come later.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#appearance-goals",
    "href": "graphics-design.html#appearance-goals",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Appearance Goals",
    "text": "Appearance Goals\nSpecifically, how exactly should a graphic look? There are so many choices: color, size, text and more. What are best practices for creating something beautiful, that represents the data honestly, and is easy to understand?\nThis section will provide some rules of thumb to help you Evaluate statistical graphics. It will also teach you to spot common problems and suggest ways to fix them, allowing you to provide constructive critique (to yourself or to others!) about how to Elaborate and refine data visualizations.\n\nYou still have your freedom!\nAs you digest all these rules and tips, you may wonder: “Do I have to always obey every one?” Well…No, of course not. Be creative!\nSometimes it’s OK to break these rules when you have thought it through and with a good justification.\nA good justification means that in your particular case, breaking a certain rule will make your graph more informative, easier to understand, or better at telling the story you’re highlighting.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#learning-objectives",
    "href": "graphics-design.html#learning-objectives",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis section will give you some basic tools to:\n\nGraph data with integrity, avoiding misleading design choices\nTell the right story, including elements that emphasize your main finding and make your figure easy to interpret at a glance",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#bye-junk",
    "href": "graphics-design.html#bye-junk",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Bye, Junk!",
    "text": "Bye, Junk!\nOur first principle is: if it doesn’t need to be in your graph, it shouldn’t be there. Keep things as simple as possible. What are some justifications for a need to include an element in a plot?\n\nIt is crucial to the story you are telling, or the research question you are answering.\nIt emphasizes your main point. For example, some plots may not need color, and in others it may add crucial visual contrast to highlight a main point.\nIt makes the graph easier to read and understand\nIt makes the main message of the graph more memorable\n\nIf you need it, include it, but if not, keep it simple!\nImagine you are using very expensive ink to print every element of the graph. Is every drop of in you’re using really worth it? If not, take it out. As influential data visualization thinker Edward Tufte put it in The Visual Display of Quantitative Information,\n\nA large share of ink on a graphic should present data-information, the ink changing as the data change. Data-ink is the non-erasable core of a graphic…\n\nIn other words, don’t let annotations, labels, grids, etc. overwhelm the visual impact of your data – Don’t do this:\n\nCartoon source: https://freshspectrum.com/data-ink-ratio/\n\nCheck: Critique the Pie\nThe figure below, from a Forbes article on mobile operating system crashes, is pretty awful.\n\nWhat is one main conclusion from the graph above? (It’s pretty confusing to interpret, so you may have to study carefully to find something…)\nNow that you have identified one main conclusion from the graph, what is one element of the plot that obscures that conclusion, is NOT necessary, and could be removed to improve the plot? Answer constructively - as if the person who made the plot was incredibly smart and someone you admire, and to whom you wanted to be kind but helpful.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#proportional-ink-and-axis-scales",
    "href": "graphics-design.html#proportional-ink-and-axis-scales",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Proportional Ink and Axis Scales",
    "text": "Proportional Ink and Axis Scales\nFor bar graphs and other filled-area plots, the area of a graphic element should correspond to the proportion of the data it represents.\nThis principle might be violated if the lower limit of a count/frequency axis is not 0. If you’re showing the number or proportion or percentage of observations in categories, it’s usually best to set the lower limit of the axis to 0.\nIf you don’t start at 0, make sure the “at a glance” interpretation of your figure is not misleading!\nA similar principle applies for plots of quantitative variables if they use filled areas to represent numeric values and there is some value (zero or another threshold value) that is an important reference point.\nFor optional additional detail on choosing honest axis limits and more examples, check out the blog post by the excellent but (sorry) profanely named Calling B*** project.\nAn example from Chapter 17 of C. Wilke’s *Fundamentals of Data Visualization shows the “Stock price of Facebook (FB) from Oct. 22, 2016 to Jan. 21, 2017”:\n\n\nWhy is the Facebook figure above ‘bad’? Mark all correct answers as TRUE.\nThe graph suggests that Facebook stock value dropped dramatically around November 1, 2016. But it actually only lost about 10% of its value. TRUEFALSE\nThe grey area should be plain white, not colored in. TRUEFALSE\nThe stock price values appear too jagged and should be smoothed out. TRUEFALSE\nThere are some stock price values that appear to be outliers and should be removed. TRUEFALSE\n\n\n\nClick for explanations of solutions above.\n\n\nIn this case, it would probably be less misleading to use a y axis ranging from 0 to 135 instead of 110 to 135.\nA line chart could also be used here (without filling in the grey area), but using the color emphasizes the values rather than the trend over time.\nWe can not just change data values to make them look how we like. If we wanted to emphasize the smoothed trend over time, we could show the smoothed curve atop a scatter plot of the original data points, which would present the desired message without altering the data. If we did average over time before plotting, it should be clearly noted (for example, “Weekly average Facebook stock prices…”).\nEven if there are outliers (there are not extreme ones here), it is only acceptable to exclude them from analysis if you can verify that they are errors.\n\n\nConsider two ways to try to “fix” the figure (again from Wilke).\n\nA.\n\n\n\nB.\n\n\nWhy is the Facebook figure above ‘bad’? Mark all correct answers as TRUE.\nWhich of the figures above solves the proportional-ink problem with the Facebook stock price plot?\n\nA does it! TRUEFALSE\nB does it! TRUEFALSE\nActually maybe both of them do it…TRUEFALSE\nNeither of them solves the problem. TRUEFALSE\n\n\n\n\nClick for explanations of solutions above.\n\nA accurately shows the reduction in stock price as a proportion of the initial price. But what about B? Some would say B accurately shows the amount of value lost (and then regained) over time. Others would argue that it is as bad as the original, for similar reasons - the graph shows the absolute lost value, not the proportion lost value. So B may be OK, or bad, depending on your perspective.\nSome would say B accurately shows the amount of value lost (and then regained) over time. Others would argue that it is as bad as the original, for similar reasons - the graph shows the absolute lost value, not the proportion lost value.\n\n\n\nCounts or Proportions or Percentages?\n\nFor categorical data, should you show the Count in each category, or the Frequency (proportion or percent)? It depends!\n\nWhen you want to compare by category across groups, and the total number in each group varies, frequency works well\nIf representing differences in sample size between groups is crucial, counts work better",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#grids-and-boxes",
    "href": "graphics-design.html#grids-and-boxes",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Grids and Boxes",
    "text": "Grids and Boxes\nShould your graphics include boxes, axis lines, and grid lines?\nWell, it depends…\n\nRemoving unnecessary axes, grids, and labels yields a cleaner plot that may be easier to take in at a glance – there is less to distract from the main story\nBut… omitting needed baselines, tick marks, gridlines, and labels can cause confusion and make it hard to identify categories or estimate numeric values\nScientific graphics usually need axis lines, with tick marks\nIf a viewer will need to refer to an axis to estimate heights of bars or locations of points, then consider using gridlines for that axis.\nInstead of an entire grid, it may be more effective to include single lines indicating important threshold values\nConsider using a color that nearly blends into the background for grid lines, so that they detract as little attention as possible from the data\n\n\nExample\nStorytelling with Data provide an example of a cluttered figure where the trend over time pops out more as unnecessary grids and boxes are made less visible, then removed:\n\nOptionally, if you’d like more examples, read S. Few’s 3.5-page article on when grid lines are helpful.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#text-elements-titles-labels-size",
    "href": "graphics-design.html#text-elements-titles-labels-size",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Text Elements: Titles, Labels, Size",
    "text": "Text Elements: Titles, Labels, Size\nWhen using text in a figure, ensure it is easy to read. Make sure no unneccessary text is included.\n\nDefault size of text in figures produced by statistical software is almost always too small. Make sure your text is big enough to be easily legible in the context where you will present it (on the page in a report, on a slide for a presentation, etc.)\nOther than the title of the vertical (y) axis, all the text in a plot should be horizontal. This makes it easier to read.\nAxis labels should be self-explanatory\n\nViewers should be able to guess what they mean accurately without looking at anything but the figure\nUse words instead of numeric codes or cryptic abbreviations\n\nAxis labels should also be as short as possible while remaining easy to understand\nEvery plot should have a title. Sometimes this might be a literal title at the top of the graph, but those are relatively rare. More often in scientific work, a text caption appears below the figure. The first phrase/sentence of the caption acts as the figure’s title\n\nRemember the melanoma rates over time figure from the last section?\n\n\nWhat helpful changes did Storytelling with Data make to the text labels as they improved the figure?\nThe x axis labels are rotated so they are horizonal. TRUEFALSE\nThe title color is changed to blue and the axis labels to grey. TRUEFALSE\nThe box around the plot is removed. TRUEFALSE\n\n\n\nClick for explanations of solutions above.\n\n\nRotating axis labels so they are horizontal is generally an improvement. To make this happen, the number of tick marks and labels on the x axis was also reduced. Notice the labels are much easier to read.\nThe color changes helped too. The blue links the title with the trend it describes, and the grey makes the axis titles less prominent and lets the viewer focus on the data. Continue to the next section for more on using color…\nThe box is gone, and it is a big improvement to the plot! But technically, you were asked about changes to the text labels…",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#using-color",
    "href": "graphics-design.html#using-color",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Using Color",
    "text": "Using Color\nColor - used with care - can be an incredibly effective part of a data visualization.\n\nEnsure your color choices highlight the story you want to tell\n\nConsider using black and grey to help some elements fade into the background - for example, grid lines and labels that must be present but aren’t the most important elements\n\nChoose color combinations that look good and are distinguishable by color-blind viewers and in greyscale\n\nDefault to using pre-defined color palettes provided by your software rather than choosing colors manually\nConsider being redundant - use size and/or shape as well as color to indicate groups.\n\nUse color consistently. Example: if “young” cases are red in one graph, don’t use red for “old” in the next graph\n\nThe video below, from Storytelling with Data, gives explanations and examples.\n\nIf you have time, watch from 11:48 to 28:41 (about 17 minutes). This segment will play automatically in the clip below.\nIf you’re in a rush, the most important sections (about 10 minutes) are:\n\n13:57 - 15:12 (Sparing use of color)\n18:44 - 21:25; see also the infographic of color in culture\n22:25 - 23:10 (Color blindness - to view your graphs as someone with color blindness would, take a screen shot and try the simulator online\n23:50 - 28:41 (Consistency)\n\n\n\n\n\nWhich of the following are lessons from the Storytelling with Data video on Being Clever with Color? Mark all correct answers “TRUE”.\nColor grabs attention. TRUEFALSE\nColor signals where to look. TRUEFALSE\nColor should be used sparingly. TRUEFALSE\nToo much color, and everything is highlighted - the viewer does not know what to pay attention to. TRUEFALSE\nColor can show quantitative values, too, not just categories. TRUEFALSE\nColors have tone and meaning. TRUEFALSE\nNot everyone can see colors. TRUEFALSE\nUse color consistently. TRUEFALSE\nSimple black and white is always the best choice. TRUEFALSE\n\n\n\nClick for explanations of solutions above.\n\n\nThe human eye is naturally drawn to colors.\nSince color grabs attention, we expect it to direct us toward the most important stuff that is worthy of our attention.\nBut…Too much color, and everything is highlighted - the viewer does not know what to pay attention to.\nAlso, remember that the meaning and interpretation of colors varies by culture.\nSince some people can not see color, use color-blind friendly palettes and redundant coding (shape, text) where possible without cluttering the figure.\nInconsistent use of color can be confusing and distracting.\nSometimes black and white is great - but often color helps you tell a story!",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#when-things-overlap",
    "href": "graphics-design.html#when-things-overlap",
    "title": "Graphics: Design Effective Visualizations",
    "section": "When Things Overlap",
    "text": "When Things Overlap\nEspecially when graphing variables with long category values, you may end up with ugly, illegible overlapping labels. Two easy solutions, in order of preference, are to switch x and y coordinates so the “long” labels are on the y axis (in R, this resizes the plot area so that labels fit); or, to rotate the too-long labels, which eliminates the overlap but makes them harder to read than horizontal text.\nSometimes a title or axis label is too long and runs off the edge of the figure. Using a smaller font is not often an ideal solution. If you can’t just use a shorter label, consider adding line breaks.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#critique-practice",
    "href": "graphics-design.html#critique-practice",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Critique Practice",
    "text": "Critique Practice\nTry using what you have learned to provide a constructive critique of an example.\nConsider the graphic below. At a glance, what do you think it means? Looking more carefully, what do you notice?\n\nPause to think: What changes, if any, would you suggest to the figure’s creator to make it clearer and easier to understand? Be sure to be constructive - gently explain any problems and suggest solutions.",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#video-review",
    "href": "graphics-design.html#video-review",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Video Review",
    "text": "Video Review\nWow, that was quite a lot of information! If you could use a brief review from a different point of view, check out the optional video from Kristen Sosulski",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#recap-reflect-12-tips",
    "href": "graphics-design.html#recap-reflect-12-tips",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Recap & Reflect: 12 Tips",
    "text": "Recap & Reflect: 12 Tips\nThe 4-minute video below summarizes design principles for data visualization in the form of 12 tips.\nAs you watch, make note of one or two tips that strike you (you’ll report your thoughts in the next section). Is there one that nicely summarizes an idea introduced earlier in the section? One you’re not sure about? One that you think is incredibly important? One that makes you say “Aha! Now I see why I loved/hated that visualization!”",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#pause-for-reflection",
    "href": "graphics-design.html#pause-for-reflection",
    "title": "Graphics: Design Effective Visualizations",
    "section": "Pause for Reflection",
    "text": "Pause for Reflection\nTake a moment to reflect on what you learned. Which Tip do you remember most clearly, think is most important, or want to challenge? Consider making a few notes for yourself for the future (you’ll have to make and critique plenty of graphics in your homework assignments).",
    "crumbs": [
      "Home",
      "Visualization",
      "Designing Data Visualizations"
    ]
  },
  {
    "objectID": "lm-plan.html",
    "href": "lm-plan.html",
    "title": "Model Planning for Inference",
    "section": "",
    "text": "We will spend most of this course learning to fit different kinds of regression models. What are they?\nWe will learn as we go. But as a starting point…\n\nIn a regression model, we have one main response or outcome variable of interest\nWe want to assess whether our response variable is or isn’t associated with (some of) a suite of possible predictor variables (also sometimes known as covariates but we’ll avoid the word “covariates” because it sometimes means slightly different things in different disciplines).\n\nEssentially, we are looking to quantify relationships between our response and predictor(s) – accompanied by appropriate measures of the uncertainty of our estimates.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#preface-whats-a-model",
    "href": "lm-plan.html#preface-whats-a-model",
    "title": "Model Planning for Inference",
    "section": "",
    "text": "We will spend most of this course learning to fit different kinds of regression models. What are they?\nWe will learn as we go. But as a starting point…\n\nIn a regression model, we have one main response or outcome variable of interest\nWe want to assess whether our response variable is or isn’t associated with (some of) a suite of possible predictor variables (also sometimes known as covariates but we’ll avoid the word “covariates” because it sometimes means slightly different things in different disciplines).\n\nEssentially, we are looking to quantify relationships between our response and predictor(s) – accompanied by appropriate measures of the uncertainty of our estimates.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#section-learning-outcomes",
    "href": "lm-plan.html#section-learning-outcomes",
    "title": "Model Planning for Inference",
    "section": "Section Learning Outcomes",
    "text": "Section Learning Outcomes\nThis section, we will learn strategies for planning a regression model - how many variables, and which ones, should be included as predictors? How does an analyst decide? What principles underlie these decisions?\nBy the end of the section you will:\n\nDefine a causal diagram, and the variable types that can be depicted in one\nUse a causal diagram to determine which variables to include as predictors in a regression model\nApply the n/15 rule-of-thumb in model planning, to determine how many coefficients can be reliably estimated with a given dataset\nCombine a causal diagram, the n/15 rule, and a modeling goal to articulate a well-reasoned plan for a multiple linear regression model\n\nAlong the way, we will also review some introductory stat material on study design and types of experiments, to make sure we are all have the same vocabulary in place. You won’t be directly assessed on this material and so it is marked as optional.\n\n\n\n\n\n\n\n\n\nComic from xkcd",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#text-reference",
    "href": "lm-plan.html#text-reference",
    "title": "Model Planning for Inference",
    "section": "Text Reference",
    "text": "Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Ch. 1.4-1.6\nEcological Models & Data in R Ch. 9\nCourse Notes Chapter 1\nStatistical Modeling: A Fresh Approach Chapter 18\nSee also: A biologist’s guide to causal inference\n\nIt’s suggested that you read these chapters after doing this tutorial, with particular focus on the topics you found most challenging.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#sampling-strategies-optional",
    "href": "lm-plan.html#sampling-strategies-optional",
    "title": "Model Planning for Inference",
    "section": "Sampling Strategies (optional)",
    "text": "Sampling Strategies (optional)\nAlmost every dataset contains information on a sample of cases from a larger population.\n\n\n\n\n\n\n\n\n\nAnd if the sample is biased – that is, not representative of the full population in important ways – then we won’t be able to make any valid inferences from it. So the process used to choose a sample is crucial to the validity of all results! What are ways to choose a sample?",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#sampling-infographic-optional",
    "href": "lm-plan.html#sampling-infographic-optional",
    "title": "Model Planning for Inference",
    "section": "Sampling Infographic (optional)",
    "text": "Sampling Infographic (optional)\nHere’s an infographic version of the main ideas here, in infographic form. This infographic includes a couple additional sampling approaches that are used in human health studies, and also introduces one more bit of terminology: probability vs. non-probability sampling. In probability methods, there is some element of random selection that is used; in non-probability samping, there is not.\nYou can also view the infographic on the web.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#study-design-optional",
    "href": "lm-plan.html#study-design-optional",
    "title": "Model Planning for Inference",
    "section": "Study Design (optional)",
    "text": "Study Design (optional)\nA crucial distinction is between two broad classes of research studies: observational studies and experimental studies.\nSide note: many students in the sciences find that the statistical definition of an “experimental study” doesn’t mesh perfectly with the definition of an “experiment” that they learn in science courses. If it helps, you can think of “experiment” keeping the colloquial meaning it has in your field, and think of “experimental study” as a technical statistical descriptor of a study (as we will learn it today).\nYou probably have prior knowledge or a good intuitive guess as to the difference between studies that are observational and experimental. We’ll review key concepts more later on, but to start, take a chance to assess your existing hunches. Study the infographic below with this distinction in mind: Does the infographic confirm, modify, or add nuance to your initial idea of how “observational” and “experimental” studies differ? As a bonus, the image also details a few kinds of studies within each broad class, and orders them in terms of the strength of evidence they can provide.\n\n\n\n\n\n\n\n\n\n\nAn Alternative\nYou may spend less time with this one - but here’s an alternative infographic covering much of the same information. It’s organized differently, but consistent - dive in if you’d like to review again!\n\n\n\n\n\n\n\n\n\nOne thing to notice in this version is the emphasis on randomized experimental studies like clinical trials as a source of possible causal conclusions – the NIH notes that a randomized experiment has the best chance of allowing researchers to conclude that one thing causes another, while that is difficult to impossible in observational work. Why?\n\n\nWhy Randomization?\nRandomization is a key tactic for ensuring the results of an experimental study are reliable, and providing a strong basis for concluding that a particular treatment or intervention causes a certain outcome or result.\nRandomization means participants are assigned randomly to different “treatments;” they are randomly chosen to receive different assigned values of the key variable(s) that researchers are controlling or manipulating. If this procedure is not followed, then the different treatment groups may differ systematically in some important feature (like gender, motivation, initial health status - anything that might influence the outcome). Here’s one more video that explains the issue quite clearly, in the context of human randomized controlled trials:\n\n\nYou can also watch it outside the tutorial.\n\n\nRandom SAMPLE vs RANDOMIZATION\nCareful! Many students confuse random sampling with randomization, or use one term when they mean the other. Check your understanding: which is which?\n\n\nWhat is the difference between random sampling and randomization?\n\n Random sampling has to do with how cases to study are randomly chosen from the population, while randomization has to do with how already-chosen participants are randomly assigned to experimental treatment groups. Randomization has to do with how cases to study are randomly chosen from the population, while random sampling has to do with how already-chosen participants are randomly assigned to experimental treatment groups. It depends on the context; they are very similar\n\n\n\n\n\nClick for explanations of solution above.\n\nThe words seem similar, but are distinct technical terms!\nRandomization has to do with how values of a “treatment” variable of special interest are assigned: randomly!\nRandom sampling means that the individuals (whether they are people, places, or things…) in the study were selected from the population at random.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#observational-vs-experimental-studies-optional",
    "href": "lm-plan.html#observational-vs-experimental-studies-optional",
    "title": "Model Planning for Inference",
    "section": "Observational vs Experimental Studies (optional)",
    "text": "Observational vs Experimental Studies (optional)\nWe mentioned earlier that the key distinction for us will be between observational and experimental studies. Hopefully you’ve already started to form a mental map of the differences. Here’s one of the most clear, concise, complete statements of the main ideas I’ve ever heard:\n\n\nYou can also watch outside this tutorial.\nAnd now, a one-minute review of the basics:\n\n\nYou can also watch outside this tutorial.\nBy now, you should be able to provide a short definition of each study type and note key differences between observational and experimental studies.\n\nEven more\nIf you would like to know more about some of the other study types featured in the infographics but not discussed in detail so far, you can check out the optional video below:\n\n\nYou can also watch outside this tutorial.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#model-planning-motivation",
    "href": "lm-plan.html#model-planning-motivation",
    "title": "Model Planning for Inference",
    "section": "Model Planning Motivation",
    "text": "Model Planning Motivation\nJust as we imagine before we start coding to create graphics, we ought to think before we start fitting models.\nTraditional ways of interpreting statistical results are premised on the idea that you made a plan, got some data, fitted the model you planned, and want to draw conclusions.\nIf, instead, you got data, scrutinized the data, fitted lots of different models, and now want to report results from the one that fitted best…well, generally things tend to go wrong. This is especially true if you use the data to lead you from a more complex to a simpler model. As Harrell (2015) points out in section 4.3, the problems are huge:\n\nUncertainty underestimated (overconfidence: standard errors and confidence intervals too small, \\(R^2\\) too big, unfounded confidence that associations are real when they may not be).\nSpurious relationships look important and slope estimates are biased high\nIf testing hypotheses, p-values too small\n\nHow can we avoid these problems? Some more insight will come when we consider model assessment and selection in future sections. For now, we need to remember:\n\nFitting and interpreting one well-considered, sensible model is prefereable to trying many things and then trying to choose among them later.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#response-and-predictors",
    "href": "lm-plan.html#response-and-predictors",
    "title": "Model Planning for Inference",
    "section": "Response and Predictors",
    "text": "Response and Predictors\nA regression model is our attempt to quantify how a response variable of interest changes when a set of predictor variables change.\nSo, to begin, we need to identify our (one) response variable – the thing we are most interested in measuring or predicting or describing or understanding.\nThen, we need to identify a set of predictor variables that we expect to be associated with changes in the response. (If we are planning an experiment, they should be variables we can collect data on; if working with data already collected, they must be in or derived from the data available.)\nHow do we choose which predictors to include, and how many?",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#expertise",
    "href": "lm-plan.html#expertise",
    "title": "Model Planning for Inference",
    "section": "Expertise",
    "text": "Expertise\nFirst, rely on experts and previous experience. If you know the context of the problem well, you have a good sense of the predictors that will be of interest. If you don’t, then you should consult experts (or published work on the topic).\nThere are also practical limits on the number of predictors you can reasonably consider, given a dataset.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#dataset-size-n15-rule",
    "href": "lm-plan.html#dataset-size-n15-rule",
    "title": "Model Planning for Inference",
    "section": "Dataset Size (n/15 rule)",
    "text": "Dataset Size (n/15 rule)\nOne important consideration, when planning a regression model, is: How many predictors can I reasonably include?\nIt depends on the size of the dataset: it takes several observations to get a good estimate of any statistics, so it makes sense that fitting a model with lots of predictors will require a bigger dataset. Each additional observation may add a little bit more capacity for fitting a more complex model.\nAnd if you try to fit too many, the chances of overfitting increase. Overfitting is when you model noise as well as signal, capturing in your model apparent relationships that actually exist only in the current dataset, not in reality.\nFor linear regression, Harrell (2015, Chapter 4.6) offers a rule of thumb: the number of parameters being estimated, \\(p\\), should be less than \\(\\frac{n}{10}\\) or \\(\\frac{n}{20}\\). To give just one standard rule of thumb, we should aim for \\(p &lt; \\frac{n}{15}\\). \\(n\\) is the sample size (number of rows in the dataset).\nThis “n/15 rule” is a very rough rule of thumb - sometimes it seems you can get away with estimating a few more parameters than it says, and sometimes fewer (particularly in the case of categorical predictors where the observations are not evenly distributed across combinations of categories). But it gives us a reality check and a starting point for planning.\nRemember, n/15 is a ceiling – and upper limit on the number of parameters you could estimate. It’s not a goal! You may have less.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#causation-revisited",
    "href": "lm-plan.html#causation-revisited",
    "title": "Model Planning for Inference",
    "section": "Causation Revisited",
    "text": "Causation Revisited\nIn most intro stat courses, students learn to repeat statements like: “Correlation doesn’t imply causation, and only randomized experimental studies can draw causal conclusions.”\nWell…kind of.\nIn data science, big observational datasets (collected in the absense of a structured study design) are really common. There are also many scenarios of interest where randomized experiments are just not possible on practical and ethical grounds.\nSome blatant examples: experiments in which people were randomly assigned to dislocate their shoulders to investigate factors influencing recovery, or start smoking to see if they get cancer, or expose themselves to someone with a contagious disease to see if they become ill. In many situations, observational work is the only real option. What’s a researcher to do?\nIn recent decades the field of causal inference has made great strides in thinking intelligently about how best to make the most reliable conclusions possible about cause and effect, when observational data is all you have. To begin to understand, we have to define a few terms: direct causation vs. indirect causation, and three alternative situations: confounders, mediators, and colliders. This field is new and technical, but the next video is about the most concise and clear primer I know of (with concrete examples).\n\n\nYou may also watch it outside this tutorial.\n\nMediators, Moderators, Precision Covariates\nOK, wait a sec. The video didn’t cover mediators! A mediator is another term for what the video called an “indirect cause” – a link in the middle of a causal chain. The mediator explains or is part of the process by which an upstream cause leads to an effect.\nThere are two other variable types it may be useful to name, too:\n\nPrecision covariates affect the response variable of interest, without having any causal links at all to the “main” predictor of interest. We include them in models when we can, to get more precise estimates.\nModerators interact with the main predictor of interest. The cause-effect relationship between the main predictor and the response varies depending on the value of the moderator variable. (Moderators might additionally affect the predictor and/or the response directly, so they may share some features with precision covariates and confounders.)",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#whats-a-causal-diagram",
    "href": "lm-plan.html#whats-a-causal-diagram",
    "title": "Model Planning for Inference",
    "section": "What’s a Causal Diagram?",
    "text": "What’s a Causal Diagram?\nA causal diagram is a picture mapping the causal relationships between key variables. They are used when researchers are interested in quantifying causal relationships between variables – not just, “is X associated with a change in Y?” but “does X cause a change in Y?”\nTo make causal conclusions with confidence outside the context of a randomized, controlled experimental study is a big challenge, and mapping out starting assumptions about relationships between variables is just the first part.\nEven if we are not necessarily trying to make causal conclusions (we won’t be here, with observational data), when you model relationships of several variables, a causal diagram helps you make smart, thoughtful choices about the ones you include in your model and the ones you leave out.\nThe diagram surfaces your starting assumptions about relationships between variables. It makes your assumptions transparent (to others and to you!) and guides choices about what to include/exclude from a model.\nPeople also call causal diagrams DAGs (for Directed Acyclic Graphs) since they are a specific application of that [broader] type of mathematical graph. So, every causal diagram is a DAG, but in math there can be DAGs that are not statistical causal diagrams.\nIn a causal diagram each box is a variable, and arrows connect causes to effects (with the arrowhead pointing from the cause to the effect).\n\n\n\n\n\n\n\n–&gt;",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#variable-types",
    "href": "lm-plan.html#variable-types",
    "title": "Model Planning for Inference",
    "section": "Variable Types",
    "text": "Variable Types\nIn a multi-variable analysis, there is often one key variable of interest (measuring “response” or “outcome” or “effect”) and another one which may influence it, and is the focus of greatest research interest (a key “predictor” or “cause”).\nBut there are generally other variables in the mix that are somehow relevant to understanding the predictor – response relationship. How can we classify (and diagram) them?\nIn all examples below, X is the predictor of greatest interest and Y is the response. (A is the other variable.) The definitions below all depend on a specific X and Y having been chosen: the other variable types are defined relative to the X-Y relationship.\n\nPrecision Covariate\nHere’s an example of A acting as a precision covariate. Precision covariates are also known as competing exposures.\n\n\n\n\n\n\n\n\n\nBased on the diagram, can you explain in words what a precision covariate is?\n\n\nClick for one definition…\n\nA precision covariate affects only the response variable.\n\nBest practice is to include precision covariates in a model trying to measure the size of the effect of X on Y, to get the most precise estimates possible…but if you don’t, it won’t bias your estimate of the size of X’s influence on Y. Can you explain why?\n\n\nMediator\nHere are two (slightly different) examples of A acting as a mediator. Based on the diagram (and the code), explain in words what a mediator is.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the diagrams, can you explain in words what a mediator is?\n\n\nClick for one definition…\n\nA mediator is part of a chain of causes and effects, between the main predictor of interest and the response.\n(You may also see mediation chains elsewhere in a diagram/scenario…for example, a chain of three causes/effects that together act as a precision covariate.)\n\nHow is this different from a precision covariate?\n\n\nHmm, not sure – show me how it’s different!\n\nA mediator is part of a chain of causes and effects, between the main predictor of interest and the response.\nIt affects the response, but unlike a precision covariate, it’s also affected by the predictor of interest.\n\nIf you have a situation like the first picture above, where there are no branches/only one single path from your predictor to the response, include only your predictor and that’s it.\nBUT if there are mediator(s) and if there are branches in the path(s), like the second picture above that’s when you have options.\nIf you want to estimate the total effect of the upstream predictor of interest, exclude the mediator(s). If you want to distinguish and quantify effects along several pathways (via the mediator in question, and another way), then include one variable along each branch whose influence you want to measure.\nBoth can be valid. Researchers have to choose which they want to do.\n\n\nConfounder\nBelow is another causal diagram, in which A is a confounder.\n\n\n\n\n\n\n\n\n\nCan you define “confounder” in words?\n\n\nClick for one definition…\n\nA confounder affects both the predictor and the response.\n\nIf there is a confounder present and you do not include it in your model, then you may wrongly conclude X causes Y.\nSo best practice is to include all confounders in a model.\n\n\nCollider\nCollider bias caused by collider variable A looks like:\n\n\n\n\n\n\n\n\n\nA collider is affected by both the predictor and response. (Their influence arrows “collide” at the collider…)\nIncluding a collider in a model designed to measure association between X and Y can induce association where there is actually none so best practice is to make sure you do not include colliders in models.\n\n\nModerator\nA variable that moderates (increases or decreases the size of) the causal link between two other variables is a Moderator.\n(If you have studied interactions before in the context of regression models, a cause and a moderator interact to influence the effect.)\nUnfortunately in our diagrams there is not really an adequate way to draw moderators – for good reasons – but here’s a diagram (from Yoon 2020) to help get a sense of the situation:\n\n\n\n\n\n\n\n\n\nIf you think you have a moderator variable, best practice is to include it in your model interacting with X. (We will return to interactions in much more detail later on.)\n\n\nMore\nThere are more details and more complicated scenarios, of course. This is all you need to master to start with. If you want to explore more, check out https://cran.r-project.org/web/packages/ggdag/vignettes/bias-structures.html",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#more-reference-drill",
    "href": "lm-plan.html#more-reference-drill",
    "title": "Model Planning for Inference",
    "section": "More Reference & Drill",
    "text": "More Reference & Drill\nCheck out the materials and interactive questions at: http://dagitty.net/learn/graphs/roles.html",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "lm-plan.html#summary-causal-diagrams",
    "href": "lm-plan.html#summary-causal-diagrams",
    "title": "Model Planning for Inference",
    "section": "Summary: Causal Diagrams",
    "text": "Summary: Causal Diagrams\nSoooo….what? How will this affect our work? Well, we will often be interested in the association or relationship between two key variables, a potential cause or “predictor” x and a potential effect or “response” y. Unless we have data from a randomized experimental study, we need to be clever about which other variables to include in our analysis to get the best, most accurate estimate of the x - y relationship:\n\nIf there are any confounders we should include them in our models and analysis. This is often called controlling for the confounders’ effects.\nIf there is a mediator, you may have a choice.\n\nIf there are no branches/only one single path from your predictor to the response, include only your predictor and that’s it.\nIf there are branches in the path, that’s when you have options. If you want to estimate the total effect of the upstream predictor of interest, exclude the mediator(s). If you want to distinguish and quantify effects along several pathways (via the mediator in question, and another way), then include one variable along each branch whose influence you want to measure.\n\nIf there are any colliders they should not be included in our models and analysis - including them would actually reduce the accuracy of our assessment of the x - y relationship\n\nSeems simple enough, right? There’s a catch. To decide whether something might be a collider or a confounder, you have to rely on your knowledge or belief about what causes what. Even experts don’t always agree. There is not usually a definite right answer that is easy to agree on. This is one thing that makes causal inference with observational data so hard, and contentious!\nBut closing your eyes to the issue and skipping the step of making a causal diagram doesn’t solve anything at all. Whatever model you fit is consistent with some causal diagrams, and not others. So by drawing the diagram as best you can, and designing your model accordingly, you are being transparent and explicit about your assumptions instead of keeping yourself and your audience ignorant that those assumptions exist, and have consequences.\nMy personal advice is: if in doubt, and if the data allow it, control for all the potentially important variables you can. The exception, of course, if is you are sure or quite sure the variable is a collider - in that case you definitely MUST exclude it.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Planning for Inference"
    ]
  },
  {
    "objectID": "quarto-how-to.html",
    "href": "quarto-how-to.html",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "",
    "text": "While you work through this tutorial, you will create a Quarto (.qmd) document.\nQuarto lets you combine R code, output, and text in a single document that can be rendered in HTML, PDF, Word and more formats.\nIt’s like magic: you save all your text and R code in a simple file; when you’re ready, push a button and it’s compiled into an output document with nicely formatted text, code (optional to include, but for this class you always will), and all the figures and tables generated by your code.\nSince all the data analysis and results are automatically included in the compiled output document, your work is reproducible and it’s easy to re-do analysis if the data change, or if a mistake is uncovered.",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#instructions",
    "href": "quarto-how-to.html#instructions",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "",
    "text": "While you work through this tutorial, you will create a Quarto (.qmd) document.\nQuarto lets you combine R code, output, and text in a single document that can be rendered in HTML, PDF, Word and more formats.\nIt’s like magic: you save all your text and R code in a simple file; when you’re ready, push a button and it’s compiled into an output document with nicely formatted text, code (optional to include, but for this class you always will), and all the figures and tables generated by your code.\nSince all the data analysis and results are automatically included in the compiled output document, your work is reproducible and it’s easy to re-do analysis if the data change, or if a mistake is uncovered.",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#reference-materials",
    "href": "quarto-how-to.html#reference-materials",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Reference Materials",
    "text": "Reference Materials\nFor more details on using Quarto, and detailed documentation, see https://Quarto.org/docs/guide/.\nQuarto and posit also provide substantial resources for learners. This tutorial is tailored to our course, including just the stuff you need and not much you won’t use frequently. But if you want even more about Quarto, you might check out:\n\nTutorials for beginners at https://Quarto.org/docs/get-started/hello/rstudio.html (Hello, Quarto! and Computations are most relevant.)\nDetailed documentation at https://Quarto.org/docs/guide/.\n\n\nOptional Video\nIf you love video introductions, consider also this 23-minute offering from posit and Mine Cetinkaya-Rundel:",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#logistics",
    "href": "quarto-how-to.html#logistics",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Logistics",
    "text": "Logistics\nTo create a .qmd file, you will have to work in RStudio (outside this tutorial environment). So, as you work on this tutorial, you will probably switch back and forth between the tutorial itself and an RStudio session on your computer or at posit.cloud.\nHistorical Note: The precursor of the Quarto document is the Rmarkdown (.rmd) document (and even older - the Sweave document). If you know and love one of those, you may use it.",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#getting-started",
    "href": "quarto-how-to.html#getting-started",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Getting Started",
    "text": "Getting Started\n\nLogging in to RStudio\nEither log in to your account at posit.cloud or open RStudio on your computer (if you installed it).\n\n\nPanels\nWhen you open RStudio, you will see at least three different panels: The Console is on the left. On the upper right are Environment, History and maybe more; on the lower right are Files, Plots, and Packages. Explore a little to try to see what is there!\nFiles shows you the files saved in your personal space on the server. You can organize, upload, and delete files and folders.\n\n\nExecuting code in R\nYou can do things in R by typing commands in the Console panel.\nHowever, working that way makes it hard to keep a record of your work (and hard to redo things if anything changes or if a mistake was made).\nFor this class, you will instead work in Quarto files, which can contain text, R code, and R output (such as figures).\nAfter you have opened a file (like an RMarkdown file) on the RStudio server, the Console panel will be on the lower left and the newly opened file will be on the top left. Let’s learn how to do it…",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#quarto-qmd-files",
    "href": "quarto-how-to.html#quarto-qmd-files",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Quarto (qmd) Files",
    "text": "Quarto (qmd) Files\n\nQuarto files are stand-alone!\nEvery Quarto file (qmd file) must be completely stand-alone. It doesn’t share any information with the Console or the Environment that you see in your RStudio session. All R code that you need to do whatever you are trying to do must be included in the qmd file itself!\nFor example, if you use the point-and-click user interface in the RStudio Environment tab to import a data file, that dataset will not be available when rendering your qmd file.\nSimilarly, if you load the mosaic package by typing in the Console window,\n\nlibrary(mosaic)\n\nmosaic functions and data will not be available to use within the qmd file.\n\n\nSo: Keep your qmd files stand-alone! (You have no choice, actually…)\n\n\n\n\nCreate a Quarto file\nIn RStudio, navigate to File -&gt; New File -&gt; Quarto Document…, or click on the white rectangle with a green circle+ :\n\n\n\n\n\n\n\n\n\nand select Quarto from the drop-down menu.\nChoose html or pdf output.\n(Why not Word? Too much temptation to make changes and do formatting after the fact in Word…which makes your work no-longer-reproducible. In qmd, you have documented everything you’ve done. If you make changes after rendering to Word, that’s not true anymore.)\n\n\nSave your qmd file\nSave your file by clicking on the disk icon at the top of the file tab (give it a clear file name like deruiter_quarto_practice.qmd).\nDo your best to avoid spaces and special characters in your file names.\nIf on posit.cloud, the file will be saved to the cloud, not to your computer. All your files will be accessible in the RStudio Files tab (lower right panel) whenever you log into RStudio, regardless of which computer you are using. You may organize them into directories (folders) if you want.\n\n\n\n\n\n\n\n\n\n\n\nRender!\nHow do qmd files actually work? What’s so cool about them?\nClick on the fat blue arrow next to the word “Render” at the top of the file window.\n\n\n\n\n\n\n\n\n\nCheck out the rendered html or pdf result, and compare it to the original Quarto file.\nWow!\n\n\nSource vs. Visual Editor\nLook to the upper right corner of your qmd file. You should see some buttons that allow you to toggle between “Source” and “Visual” editor modes.\n\n\n\n\n\n\n\n\n\nIn your own file, toggle back and forth a few times. The Source mode lets you see (and type) the straight-up markdown – which is probably nice if you’re already used to it, and annoying or mystifying if not. The Visual mode is more of a what-you-see-is-what-you-get (like the rendered version), point-and-click type interface. You may use whichever you prefer.\n\n\nPersonalize your Markdown file\nAt the top of the Quarto file, there is a section called the “YAML header”. It starts and ends with 3 dashes - - -.\n\n\nIn this part of the file, be very careful what you type: a stray space or character will lead to an error.\n\n\nThis is where you can enter an appropriate title, author(s), and date (within the quotation marks). You can also choose the format you want to render to (usually pdf or html – not in quotes).\n\n\n\n\n\n\n\n\n\nCustomize your YAML header in your own Quarto doc, and then render again to see the effect.\nMake sure you do this for every assignment! (No prof or boss likes getting submissions called “Untitled”…)",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#quarto-yaml-settings",
    "href": "quarto-how-to.html#quarto-yaml-settings",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Quarto YAML settings",
    "text": "Quarto YAML settings\n\nPDF or html?\nFor our course, you can choose to render to either an html file or a PDF file.\nSo, you’ll have either format: pdf or format: html in your YAML header. You can also try format: typst to render PDF files a bit faster (learn more about typst output format online).\nBut if you choose html, there’s an important change you have to make to the YAML header to ensure your html file is stand-alone. Meaning: you want all images, etc. to be embedded in the one file rather than stored in an accompanying folder. Otherwise, when you (say) upload the file on Moodle or email it, all the images and graphs will be omitted…yikes! Yes, embedding these makes the file larger, but if you are sharing the rendered html document, you need to.\nIf rendering to html, it is essential that you specify the setting embed-resources: true!\nSo, make sure you add embed-resources: true after the entry format: html: in your YAML header, exactly as shown below.\nMake sure to keep the spacing and line breaks just as shown.\nThe indents are each two spaces, so there are 2 spaces before html: and 4 before embed-resources:.\n\n\n\n\n\n\n\n\n\n\n\nCode tools\nNote that the YAML header shown above also had a second option activated for rendered html files: code-tools: true.\n\n\n\n\n\n\n\n\n\nWhat does this one do?\nIt adds a button “Code” at the top right of your file.\n\n\n\n\n\n\n\n\n\nIf you click it, you can view and copy the source code (basically, the contents of the original qmd file before rendering). This is not a bad option, for example for homework, as it allows me to see every detail of the settings you used and may help me troubleshoot any issues.",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#text-and-code-in-quarto",
    "href": "quarto-how-to.html#text-and-code-in-quarto",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Text and Code in Quarto",
    "text": "Text and Code in Quarto\n\nText\nThe Quarto file is where you save all the R commands you want to use, plus any text commenting on the work you are doing and the results you get. Parts of the file with a plain white background are normal text.\nYou can format the text. For example, enclosing a word in asterisks will generate italics, so *my text* in the qmd file will become my text in the PDF. Using two asterisks instead of one will generate boldface, so **my text** becomes my text. You can also make bulleted lists, numbered lists, section headers, and more. For example,\n#### Some Text\nbecomes\n\nSome Text\n(a sub-section header). Fewer hashtags make the text even larger, and more make it smaller.\nCaution! Forgetting the space after the last hashtag will format your text verbatim rather than as a header (#fail). Failing to leave a blank line before the header can also make formatting fail.\nCheck out the Quarto Markdown Basics reference at https://quarto.org/docs/authoring/markdown-basics.html for more examples of how to format text in Quarto.\nBefore moving on, try a few of the tricks you just learned in your qmd file. Make it pretty!\n\n\n\nqmd file anatomy: R code chunks\nAn qmd file can (of course!) contain one or more R code chunks. These sections of the file have a grey background onscreen. In Source mode, each one begins with\n```{r}\nand ends with\n```\nlike so:\n\n\n\n\n\n\n\n\n\nIn Visual mode you can’t see the `:\n\n\n\n\n\n\n\n\n\n\n\nHow to add a new R code chunk to your file\nTo add a code chunk to your file in Source editor mode, you have three options.\n\nYou can type in the header and footer by hand to start and end the chunk.\nYou can click on the “add chunk” button at the top right. It’s a green box with the C inside (at the top of the qmd file; choose the first option, “R”, in the pulldown) to insert an empty chunk.\nYou can use a keyboard shortcut: Windows, Ctrl + Alt + I or OS X, Cmd + Option + I\n\nWhen you click the Render button, code in code chunks will be run, and any output will be included in the document.\n\n\n\n\n\n\n\n\n\n\n\nSetup Chunk\nConsider using the first R code chunk in a qmd file to specify settings (for graphics, display, etc.). In this chunk, you can also give R permission to use certain packages (software toolkits) with\n\nlibrary(packagename) \n\nFor example, we will use the ggformula package for graphics. So, verify that the first R code chunk in your file includes the line library(ggformula).\nYou can also specify options for each R code chunk - these go at the top, prefaced by #|. A typical setup chunk for our course might look like:\n\n\n\n\n\n\n\n\n\nNotice that several packages are loaded (that we will use frequently). theme_set() is used to specify some settings for graph output, and knitr::opts_chunk$set() is used to specify whether or not to include R code in the rendered file (Yes please: use echo: true!) and specify the default figure size.\nThere are tons more options and settings, and you can explore them at https://yihui.org/renderr/options/#chunk-options.\nBut for now, if you use something like the setup chunk shown above, it should work well and have what you need for almost all work in this course.\n\n\nThe settings chunk is invisible!\nIf you look carefully at the rendered output, you will see that the setup chunk does not appear there. That’s intentional - when you load packages with library(), they often print a lot of long and pretty useless messages, which you want to omit from your rendered document.\nThis is achieved by having the setting include: false\nHowever, for our course, no chunk other than the setup chunk should have the setting “include: false” (or echo: false for that matter). Generally, anyone evaluating your coursework needs to see all the code you used, not just its output.\n\n\nClean Up\nAt this point, you probably want to get rid of all the extra content in the template.\nIf you haven’t put a setup chunk into your own qmd file…do it now! Here’s another reminder of how it would look:\n\n\n\n\n\n\n\n\n\nNext, Delete everything in the file other than the YAML header and your setup R code chunk.\nNow the clutter is gone and you have space to include your own R code and text.\n(Before going further, make sure it still renders.)",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#run-r-code",
    "href": "quarto-how-to.html#run-r-code",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Run R Code",
    "text": "Run R Code\nThere are multiple ways to run and test R code from a markdown file. Sometimes you want to render the whole file and get the PDF or HTML; other times you want to run just a specific bit of code to make sure it’s working correctly.\n\nRunning R Code from a qmd file: Render the file\nEvery time you render the file, all R code will be run automatically.\nA side note: PDF or HTML? Which is preferable?\nI think PDFs are a little more portable and a good default option, and their formatting is best for anything you are going to print out or share via email (especially with less technically inclined folks).\nHowever, later in the semester we may see how to create some pretty cool interactive graphics and/or tables in R, and these can only be rendered in HTML. For this class, you may use either one. (But not Word, remember? Because you’ll lose reproducibility…)\n\n\nRunning R Code from a qmd file: Run Menu\nYou can also use shortcuts/buttons to run specific chunk(s). Here is one way to do it (option 1): Use the Run pulldown menu at the top of the file. (Choose the option you want based on what you are trying to do).\n\n\n\n\n\n\n\n\n\n\n\nRunning Code from a qmd file: Shortcut Button\nHere is another way to use shortcuts/buttons to run only a specific chunk (option 2): Click on the green arrow at the upper right of a code chunk to run the chunk.\n\n\n\n\n\n\n\n\n\n\n\nRunning Code from a qmd file: Copy and Paste\nFinally, here’s a third way to use shortcuts/buttons (option 3):\nCopy the code you want to run, paste to the console window, and hit Enter.\n(Or, place your cursor in the line you want to run and hit ctrl + enter (Windows) or cmd + enter (Mac).)",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#downloading-files-from-rstudio",
    "href": "quarto-how-to.html#downloading-files-from-rstudio",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Downloading files from RStudio",
    "text": "Downloading files from RStudio\nYou will have to download your files if you want a copy on your own computer, or to be able to upload a copy to Moodle to turn in.\nTo download, go to the File tab, check the box for the file you want, then select More - Export. from the menu at the top of the File tab.",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#quarto-files-stand-alone",
    "href": "quarto-how-to.html#quarto-files-stand-alone",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Quarto Files Stand Alone!",
    "text": "Quarto Files Stand Alone!\nWe already covered this once, but we’re covering it again because it’s one of the most common student mistakes in qmd files!\nIf you run R code in the console or the RStudio GUI (for example, reading in a data set by pasting code into the console or using the Import Dataset button in the Environment tab), you won’t be able to use the results in your markdown file.\nAny and all commands you need, including reading in data, need to be included in the file.\nThe reverse is also true. If you run just one R code chunk in a qmd file using the “run” buttons mentioned above, or by copy-pasting into the console, you are effectively running that code in the console.\nIf R gives an error saying it cannot find a certain funtion, variable, or dataset, the most likely fix is to run the preceding code chunks (especially setup!) before the one you’re stuck on.",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#data-from-a-url",
    "href": "quarto-how-to.html#data-from-a-url",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Data from a URL",
    "text": "Data from a URL\nYou can load online datafiles in .csv format into R using the function read_csv(). The input to read_csv() is the full url where the file is located, in quotation marks. (Single or double quotes – it doesn’t matter which you choose, as they are equivalent in R.)\nFor example, we will consider a dataset with counts of the numbers of birds of different species seen at different locations in Hawai’i. It is stored at https://sldr.netlify.app/data/hawaii_birds.csv, and can be read into R using the command below.\n\nhi_birds &lt;- read_csv('https://sldr.netlify.app/data/hawaii_birds.csv')\n\n\nWhen you read in data, store it to a named object\nNote that we didn’t just run the read_csv() function – we assigned the results a name so that instead of printing the data table to the screen, R stores the dataset for later use.\n\nhi_birds &lt;- read_csv('https://sldr.netlify.app/data/hawaii_birds.csv')\n\nHere, we assigned the name hi_birds to the dataset using an “assignment arrow” &lt;- (the “arrow” points from the object toward the name).",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#data-from-google-sheets",
    "href": "quarto-how-to.html#data-from-google-sheets",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Data from Google Sheets",
    "text": "Data from Google Sheets\nThere’s also a simple way to read in data from a Google Sheet.\nFirst, go to the Google Sheet online to prepare it by “publishing it online”.\nIn the File menu, choose “Publish to the Web”:\n\n\n\n\n\n\n\n\n\nIn the pop-up window, choose to publish your “Entire Document” as a .csv file:\n\n\n\n\n\n\n\n\n\nFinally, copy the resulting link.\n\n\n\n\n\n\n\n\n\nYou can use read_csv() with this link as input to read your data into R.",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#data-from-a-file",
    "href": "quarto-how-to.html#data-from-a-file",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "Data from a File",
    "text": "Data from a File\nYou can also upload your own data file to posit.cloud, or save it to your computer if you installed R/RStudio, and then read it in to R using read_csv(). The basic process is:\n\nUse spreadsheet software to create the data table\nSave the file as a csv file\nUpload the csv file if working on posit.cloud\nUse the read_csv() function to read the file into R",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "quarto-how-to.html#r-functions",
    "href": "quarto-how-to.html#r-functions",
    "title": "Reproducible Documents with Quarto in RStudio",
    "section": "R functions",
    "text": "R functions\nAfter reading the data in, you can use R functions to have a look at it, for example:\n\nhead(hi_birds)\nglimpse(hi_birds)\nnrow(hi_birds)\n\nTry each of the lines of code above in R. What do the functions head(), glimpse(), and nrow() do? Try to figure it out based on the output they produce.\nIf you get stuck, consult R’s built-in help files. Remember, you can access the help for a function by running the code ?functionName – for example, if you want help on head(), run:\n\n?head",
    "crumbs": [
      "Home",
      "Quarto",
      "Using Quarto"
    ]
  },
  {
    "objectID": "lm-interp-more.html#section-learning-outcomes",
    "href": "lm-interp-more.html#section-learning-outcomes",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "Section Learning Outcomes",
    "text": "Section Learning Outcomes\nEarlier, we started but didn’t finish talking about model selection, and making inferences based on linear regression models.\nWhat if we had a model with several predictors, or a categorical predictor, rather than just one quantitative predictor?\nThis tutorial will give a short overview of a few common model selection tools.\n\nANOVA??\nNote that for simple one-predictor linear regression, the p-value reported at the very end of the model summary is the same as the one for the slope in the coefficient table.\nThis is only the case when there’s only one, quantitative predictor, since the test at the bottom is testing \\(H_0: \\text{all the slopes in the model are 0 for all predictors}\\). But…it’s reported along with an F-statistic – which reminds us of the one-way ANOVA possibly encountered in a previous stats course. As you later fit models with more than just one predictor, ANOVA does come up as one approach for testing hypotheses about predictor-response associations!",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-interp-more.html#case-study-gratitude",
    "href": "lm-interp-more.html#case-study-gratitude",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "Case Study: Gratitude",
    "text": "Case Study: Gratitude\nA growing body of research has documented the effects that practicing gratitude can have on people – providing not just spiritual and mental-health benefits, but even improving physical health.\n\n\n\nDataset\nWe will dive further into this topic – learning about ANOVA and some other tests along the way – via a case study where researchers tried to collect data to answer:\n“How does gratitude affect peoples’ mental and physical well-being?”\nThe data are simulated based on the results of a paper published in 2003:\n\n\n\nCan We Induce Gratitude?\nTo understand whether gratitude can cause improvements in mental and physical health and practices, the researchers needed to do a randomized study, in which they somehow controlled peoples’ gratitude levels. How did they do that?\nFirst, they recruited nearly 200 college students to participate in their study.\nThey asked participants to complete a weekly journal, writing lists of things that they were grateful for. 1/3 of study participants were randomly assigned to this group - the gratitude group.\nOther participants were asked to write about things that annoyed them instead – this was the hassles group.\nFinally, a control group – the events group – just wrote about events in the past week that had affected them (could be either positive or negative).\nBefore delving into any other questions, the researchers had to verify that the gratitude group actually felt more grateful than the other groups…\n\n\nMore Design Details\nIn addition to the journals, all the students had to complete weekly surveys about their behavior and state of mind.\nFor example, they had to state how often (on a scale of 1 to 5) they experienced each of a set of feelings over the preceding week:\n\n\n\n\n\n\n\n\n\n\n\nPulling out Gratitude\nThe researchers combined scores from the words thankful, appreciative, and grateful to assess participants’ gratitude. In our dataset, this value is called gratitude_score.\n\n\n\n\n\n\n\n\n\n\n\nThe Data\nHow do the data actually look?\n\n\n\n\n\n\n\n\n\nIt seems like perhaps the gratitude_score is higher for the gratitude group and lower for the others, but we really need statistical inference to have more confidence in our judgment of whether the difference is real or if the differences between groups could just be the result of random sampling variation.\n\n\nOur Questions\nAs Emmons and McCullough did, our first job will be to test whether the mean gratitude_score is the same for the different groups in the study – the gratitude group, the hassles group, and the events group.\nTo build up to the question we want to answer (comparing all three groups), we will consider a simplified case with only two groups: the gratitude and hassles groups. This is only for learning purposes and not something we’d do in a real analysis!\nWe also have other potential quantitative predictors in the dataset we could include in our models. We will take advantage of that to review CIs and tests for a quantitative predictor, too.\n\n\nReview: Quantitative Predictors\nFor example, we could consider a model to predict gratitude_score as a function of life_rating (a score measuring positivity about one’s life as a whole).\n\n\n\n\n\n\n\n\n\n\nlife_model &lt;- lm(gratitude_score ~ life_rating, \n                 data = grateful)\nsummary(life_model)\n\n\nCall:\nlm(formula = gratitude_score ~ life_rating, data = grateful)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.891 -1.466 -0.098  1.219  5.899 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.2008     0.8828   8.157 4.22e-14 ***\nlife_rating   0.5295     0.1806   2.932  0.00377 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.98 on 194 degrees of freedom\nMultiple R-squared:  0.04243,   Adjusted R-squared:  0.03749 \nF-statistic: 8.596 on 1 and 194 DF,  p-value: 0.003775\n\n\n\n\nReview: CI for Slope\nFind a 95% confidence interval for the slope coefficient of the life_model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution:\n\n\n\n\n\nlife_model &lt;- lm(gratitude_score ~ life_rating, \n                 data = grateful)\nconfint(life_model)\n\n\n\n\n\n\nReview: Test for Slope\n\n\nYou test the hypothesis that the slope of the life_model is zero. What is the p-value of the test?\n\n Something else 0.0424 0.00377 4.22e-14\n\n\n\n\n\nCategorical?\n\nIf we consider a categorical variable with only two categories, everything is exactly the same (except the interpretation of the slope coefficient, which now gives the difference in means between the two categories).\nFor example, we can reduce the dataset to exclude the third events group, and then model gratitude_score as a function of group to see if the gratitude and hassles groups have different average gratitude scores.\nOf course we would never normally do this! But right now, for learning purposes, we could use a categorical variable with 2 categories…and ours has three. So don’t try this filtering/subsetting at home!\n\ngrateful2 &lt;- grateful |&gt;\n  filter(group != 'events') |&gt;\n  mutate(group = factor(group))\ngf_boxplot(gratitude_score ~ group, data = grateful2)\n\n\n\n\n\n\n\n\n\n\nPractice\nFit the model and, just as in the previous example, find a 95% CI for \\(\\beta_1\\) and test \\(H_0: \\beta_1 = 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution:\n\n\n\n\n\ntwo_group_model &lt;- lm(gratitude_score ~ group, \n                      data = grateful2)\nsummary(two_group_model)\n# if you wanted to pull out just the p-value (advanced move)\ncoefficients(summary(two_group_model))['grouphassles', 'Pr(&gt;|t|)']\n\n\n\n\nEasy peasy! As we mentioned before, the only real difference between this model and the one with a quantitative predictor is that we end up with an indicator variable instead of one with many different numeric values, so the interpretation of the slope coefficient \\(\\beta_1\\) is different.\n\n\nIn our model to compare the gratitude and hassles groups, the grouphassles coefficient estimate is about -1.54. What does this mean?\n\n The mean gratitude_score for people in the hassles group is -1.54. The mean gratitude_score is 1.54 units lower in the hassles group than in the gratitude group. Something else The mean gratitude score is 1.54 units higher in the hassles group than in the gratitude group. When the amount of 'hassles' increases by one unit, the gratitude_score goes down by 1.54 units.\n\n\n\nBut what if we have a categorical predictor with MORE THAN TWO categories? We need a test that incorporates differences between all the categories at once, so the tools we have so far aren’t enough.",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-interp-more.html#hypotheses-for-anova",
    "href": "lm-interp-more.html#hypotheses-for-anova",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "Hypotheses for ANOVA",
    "text": "Hypotheses for ANOVA\nWe want to test:\n\\[H_0: \\mu_{gratitude} = \\mu_{events} = \\mu_{hassles}\\]\nIn other words, our null hypothesis is that the means of all groups are the same. (The \\(\\mu\\)s are the true population means for each of the groups.)\nThe alternate hypothesis is that at least one pair of groups has different means.\nHow do we translate this set-up into a linear regression model? We’re considering the model in which we predict gratitude_score with group:\n\ngrat_by_group &lt;- lm(gratitude_score ~ group, data = grateful)\nsummary(grat_by_group)\n\n\nCall:\nlm(formula = gratitude_score ~ group, data = grateful)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.527 -1.208 -0.128  1.160  7.016 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      9.5317     0.2383  40.006  &lt; 2e-16 ***\ngroupgratitude   1.1096     0.3369   3.293  0.00118 ** \ngrouphassles    -0.4280     0.3357  -1.275  0.20382    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.921 on 193 degrees of freedom\nMultiple R-squared:  0.1036,    Adjusted R-squared:  0.0943 \nF-statistic: 11.15 on 2 and 193 DF,  p-value: 2.612e-05\n\n\nThere are a few equivalent ways of setting this up (depending on which group is included in the intercept), but in R the code above will yield a model like:\n\\[ y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nwhere \\(x_1\\) and \\(x_2\\) are indicator variables for the gratitude and hassles groups:\n\\[ x_1 = \\begin{cases} 1 \\text{,  if group is gratitude}\\\\ 0 \\text{,  otherwise} \\end{cases}\\]\n\\[ x_2 = \\begin{cases} 1 \\text{,  if group is hassles}\\\\ 0 \\text{,  otherwise} \\end{cases}\\]\nIn this framework, \\(\\beta_0\\) is the mean for the events group. \\(\\beta_1\\) and \\(\\beta_2\\) are the differences in means between groups (gratitude and events, and hassles and events). So our intention with this ANOVA is to test \\(H_0: \\beta_1 = \\beta_2 = 0\\). If this is true, then the intercept \\(\\beta_0\\) will be the overall mean gratitude_score.",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-interp-more.html#test-stat-for-anova-f-ratio",
    "href": "lm-interp-more.html#test-stat-for-anova-f-ratio",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "Test Stat for ANOVA: F-ratio",
    "text": "Test Stat for ANOVA: F-ratio\nThe big challenge here is to come up with a test statistic – one number that measures evidence against the null hypothesis that all the group means are the same.\nHow can we define a good test statistic to measure how different more than two numbers are?\nThe optional video below will walk through a derivation of the F-ratio statistic.\n\nPart 1: What we need to measure\nNote: in this video, \\(x\\) is used for the variable whose mean is being estimated; for us it would make more sense to use \\(y\\) but I’m trusting you to make the translation…\n\n(You can also watch directly on YouTube if you prefer.)\n\n\nPart 2: How we measure it\nNote: again in this video, \\(x\\) is used for the variable whose mean is being estimated; for us it would make more sense to use \\(y\\) but I’m trusting you to make the translation…\n\n(You can also watch directly on YouTube if you prefer.)\n\n\nSampling Distribution of F, Simulated\nAll right, we can compute the F-ratio now. And we know that the bigger it is, the stronger the evidence we have against the idea that the categorical predictor is not associated with the response.\nBut…how big will it be, even if there is no association?\n\n\nIf the null hypothesis is true, what will the value of the F-ratio be?\n\n Close to 1 Very Large It depends… Close to 0\n\nIf the population group means are actually very different, with little variation within groups, what will the value of the F-ratio be?\n\n Close to 0 Close to 1 It depends… Very Large\n\n\n\n\n\nClick for more explanations…\n\nIf \\(H_0\\) is true, then SSG may be ‘close to 0’ because the group means will be very close to the overall mean. So, MSG and the whole F-ratio will also be near 0 - but since it MSG is not exactly zero, it matters a lot what the corresponding value of MSE is…\nF will be close to 1 if the null is true and the sample sizes in the groups are equal. MSE and MSG will BOTH depend on the overall variance of the data. In terms of intuition about the F ratio, though, it can be more useful to think about what happens to MSG when \\(H_0\\) is true (it gets smaller, and so so does F…). Bigger F is more evidence against \\(H_0\\).\nIf \\(H_0\\) is very wrong, then SSG will be relatively large because the group means will be quite different from the overall mean. And SSE will be relatively small, if there is little within-group variation. So the ratio will be (big / small) = VERY BIG, providing strong evidence against the null hypothesis and allowing us to conclude that the group means are NOT the same.\n\nAgain, we know that a large value of F will let us reject the null. But how big is “large”?\nWe’d need to know the sampling distribution of F (when \\(H_0\\) is true) in order to judge.\n\n\nLetting R do all the work\n\nWe won’t go into the details of the calculation here; we’ll just note that there’s an R function to automate it all.\n\n\nAnova Table (Type II tests)\n\nResponse: gratitude_score\n          Sum Sq  Df F value    Pr(&gt;F)    \ngroup      82.30   2  11.152 2.612e-05 ***\nResiduals 712.15 193                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBut notice – all the quantities that we used in our derivation of the F-statistic are right there in the ANOVA output table!\n\nThe “group” row gives the SSG and its df (and the F stat and p-value)\nThe “Residuals” row gives the SSE and its df. (Residuals is another statistical term for errors.)\nThe “Sum Sq” column corresponds to “SS” terms (“MS” terms are not in the table, but can be computed from SSX and df from the table)\nThe “F value” is the F-ratio (test statistic)\nThe “Pr(&gt;F)” is the p-value of the test\n\n(A classic intro-stat textbook ANOVA problem is to give you a partially-filled-in ANOVA results table, and ask you to fill in the missing values.)\n\nR Note: Anova(), not anova()!\nBe careful not to use anova() to carry out tests related to predictors in a regression model.\nanova() does “sequential” tests; for example, if you fit a model y ~ pred1 + pred2 + pred3, for pred1 anova() will compare the intercept-only model to the one with pred1 in it, but for pred2 it will compare the model with pred1 and pred2 in it to the one with only pred2, and for pred3 it will compare the model with all three predictors to the one without pred3. ACK!\nIn other words, with anova(), the order in which predictors are listed greatly affects the hypotheses tested, and the test results. THIS IS (ALMOST ALWAYS) NONSENSE. You want Anova() which is in the package car.\n\n\nModel Comparisons\nIn other cases, if we want to compare two particular models with the same response variable but different predictors, we can just fit them both and then compare them with the syntax below.\n\nanova(model1, model2)\n\n(Yes, little-a anova() is what you want this time.)\nWhy might we want to do this? Maybe, for example, you have two different predictors that measure a similar quantity in different ways and you want to know which one works better as a predictor?\nThis anova(model1, model2) approach also works for any of the above cases (removing parameters), if you manually fit the two models.",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-interp-more.html#akaikes-information-criterion",
    "href": "lm-interp-more.html#akaikes-information-criterion",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "Akaike’s Information Criterion",
    "text": "Akaike’s Information Criterion\nInformation theory has influenced many fields, including statistics. In 1974, Japanese statistician Hirotugu Akaike\n\nused information theory to derive a new criterion, now known as Akaike’s Information Criterion (AIC), to allow comparison of a set of statistical models fitted to a dataset.",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-interp-more.html#aic-definition",
    "href": "lm-interp-more.html#aic-definition",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "AIC Definition",
    "text": "AIC Definition\nAIC is defined as:\n\\[ \\text{AIC} = 2k - 2(\\hat{\\ell})\\]\nWhere \\(k\\) is the size of the model (number of parameters being estimated), and \\(\\hat{\\ell}\\) is the maximum value of the (base \\(e\\)) log-likelihood fuction.\n\nThe first term, \\(2k\\), inflates the AIC value, causing larger models to have worse AIC values. This is often thought of as a “penalty” on model size.\nThe second term, \\(-2\\hat{\\ell}\\), means that (better-fitting) models with higher log-likelihoods have better AIC values.\n\nR function AIC() computes the AIC for fitted lm() models.",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-interp-more.html#aic-practice",
    "href": "lm-interp-more.html#aic-practice",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "AIC Practice",
    "text": "AIC Practice\nWe can use AIC to choose the “best” of a pair (or a larger group) of models.\nFor example, before we used ANOVA to decide whether group (“gratitude”, “hassles”, and “events”) is a good predictor of gratitude score:\n\n\nAnova Table (Type II tests)\n\nResponse: gratitude_score\n          Sum Sq  Df F value    Pr(&gt;F)    \ngroup      82.30   2  11.152 2.612e-05 ***\nResiduals 712.15 193                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can make the same comparison with AIC.\nFirst, we find the AIC for the full model with the group predictor:\n\n\n[1] 817.0972\n\n\nHmmm…this number means nothing in isolation. The only way to get meaning from it is to compare it to the AIC for another model fitted to the same exact dataset and see which is better (smaller).\nDo the rest of the comparison: what is the AIC of the intercept-only model, and which model is better according to AIC?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHints…\n\n\n\n\n\nNote: working on your own dataset in a qmd file, you’d probably fit each model, give the fitted model object a name, and then use AIC(model_name)…\nAIC(lm(gratitude_score ~ 1, data = grateful))\nAIC(lm(gratitude_score ~ group, data = grateful))\n\nAIC(lm(gratitude_score ~ 1, data = grateful)) -\nAIC(lm(gratitude_score ~ group, data = grateful))\n\n\n\n\n\n\nWhat is the difference in AIC between the two gratitude models you just fitted?\n\n 17.4; the intercept-only model has lower AIC The AIC values for the two models are about the same 834.5; the model with the group predictor has lower AIC. 17.4; the model with the group predictor has lower AIC.\n\nWhich model is better, according to AIC?\n\n Model with group predictor Intercept-only model They are both about the same\n\n\n\nRemember – smaller AIC is better!",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-interp-more.html#ic-how-big-a-difference",
    "href": "lm-interp-more.html#ic-how-big-a-difference",
    "title": "More Interpretation and Inference for Regression Models",
    "section": "IC, How big a difference?",
    "text": "IC, How big a difference?\nStrictly and simply using AIC, we can say that a model with lower AIC is better (regardless of how small the difference between the AIC values is).\nIn practice, if using AIC to choose between two models, analysts often require some minimal AIC improvement to justify adding an additional parameter/predictor to a model. (Common thresholds are 2, 3, or maybe 6). So, one might not prefer a larger model unless it reduces AIC by at least 2-3 units.\nSo, if we are comparing two models (say, with and without a key predictor of interest), if the model with the predictor is better by well over 2-3 AIC units, we consider that pretty convincing evidence of an association between that predictor and the response.\nFor a lot more excellent information about practical use of AIC, the interested reader can check out the classic book by Burnham and Anderson:",
    "crumbs": [
      "Home",
      "Linear Models",
      "More Inference"
    ]
  },
  {
    "objectID": "lm-fit.html#learning-outcomes",
    "href": "lm-fit.html#learning-outcomes",
    "title": "Fitting Linear Models in R",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nThis section, we’ve already considered strategies for planning a regression model - how many variables, and which ones, should be included as predictors? How does an analyst decide?\nNow, here, we will also learn to fit the models in R and examine the results.\nBy the end of the section you will:\n\nFit a multiple linear regression to a dataset in R, using multiple quantitative and/or categorical predictors\nState the equation of a linear regression model, based on a model description or an R model summary",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#text-reference",
    "href": "lm-fit.html#text-reference",
    "title": "Fitting Linear Models in R",
    "section": "Text Reference",
    "text": "Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapter 1.4-1.6\nCourse Notes Chapter 1\nIntro to Modern Statistics Chapter 3*\nEcological Models & Data in R Chapter 9\nStatistical Modeling: A Fresh Approach Chapters 6-8",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#quick-review-of-lines",
    "href": "lm-fit.html#quick-review-of-lines",
    "title": "Fitting Linear Models in R",
    "section": "Quick review of lines",
    "text": "Quick review of lines\nConsider the line below.\n\n\n\n\n\n\n\n\n\nYou may remember that lines have equations that look like\n\\[y = mx + b\\]\nwhere \\(m\\) is the slope and \\(b\\) is the \\(y\\)-intercept (the value of \\(y\\) when \\(x = 0\\)). So we can read off an equation for our line by figuring out the slope and the intercept.\n\nIntercept and Slope\nWhen \\(x = 0\\) on the line we just looked at, \\(y = 2\\), so the intercept is \\(2\\). It’s indicated by the red dot below:\n\n\n\n\n\n\n\n\n\nThe slope is computed as “rise over run”. We can compute this by comparing the red and blue dots and doing a little arithmetic.\n\n\n\n\n\n\n\n\nAny two points on the line give the same slope. Chose two different points on the line and recompute the rise and the run. You should get the same slope.\nSo the equation for our line is\n\\[y = 4x + 2\\]\nStatisticians like to write this a different way (with the intercept first and with different letters):\n\\[ y = \\beta_0 + \\beta_1 x\\]\n\\[y = 2 + 4x\\]\nSo \\(\\beta_0\\) (read “beta zero”) is the intercept and \\(\\beta_1\\) (read “beta one”) is the slope.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#simple-linear-regression-equation",
    "href": "lm-fit.html#simple-linear-regression-equation",
    "title": "Fitting Linear Models in R",
    "section": "Simple Linear Regression Equation",
    "text": "Simple Linear Regression Equation\nIn your intro stat course (whenever that was!) you likely learned to fit simple linear regression models, and hopefully also to check the conditions that must hold for such models to give reliable results. (If you didn’t learn that condition-checking, known as model assessment, don’t worry - stay tuned for next week!) Some of the material early in our course may be review for some, but we will build on it rapidly too.\nHere, we start with a reminder of the form of the linear regression equation, which is a small elaboration of the equation of a line we just saw:\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon, \\epsilon \\sim \\text{Norm}(0, \\sigma)\\]\n\n\\(y\\) is the response variable\n\\(x\\) is the predictor (or explanatory) variable\n\\(\\beta_0\\) is the intercept. To fit the model to a specific dataset, we have to estimate the numeric value of this parameter.\n\\(\\beta_1\\) is the slope. To fit the model to a specific dataset, we have to estimate the numeric value of this parameter.\n\\(\\epsilon\\) are the residuals (or errors).\n\nThey measure the vertical distance between an observed data point (\\(y_i\\) is the \\(i\\)th one) and the predicted (or fitted) value on the line for the same predictor variable value (\\(\\hat{y}_i\\)). Much more about residuals later one!\nResiduals have a normal distribution with mean 0 and standard deviation \\(\\sigma\\) (some value that we can estimate once we have the slope and intercept: we compute the fitted values \\(\\beta_0 + \\beta_1x\\) and subtract from the observed response variable values to get the residuals, then estimate their standard deviation \\(\\sigma\\).)",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#multiple-predictors",
    "href": "lm-fit.html#multiple-predictors",
    "title": "Fitting Linear Models in R",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\nOften, statisticians consider more than one \\(x\\) variable. Why? Well, most response variables we are interested are associated with not just one but many different variables – in the language we just learned, many variables could be potential causes, confounders, mediators, or moderators (but not colliders…because we would not put those in the model).\nWith multiple predictors, the linear regression equation looks more like\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots \\beta_p x_p + \\varepsilon\\] \\[\\varepsilon \\sim \\text{Norm}(0, \\sigma)\\]\nwhere\n\n\\(\\mathbf{Y}\\) is the response variable value,\nthe \\(\\beta_0\\) is the intercept,\n\\(\\beta_1, \\dots \\beta_p\\) are the \\(p\\) “slopes”,\n\\(x\\)s are the \\(p\\) predictor variable values, and\n\\(\\varepsilon\\) are the “errors” (since real data doesn’t all fall exactly on a line).\n\nWhen we use data to estimate the \\(\\beta_i\\)’s, we may write it this way:\n\\[ \\hat Y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots \\hat{\\beta}_p x_p\\]\n\nThe hats on top of the the \\(\\beta\\)s and \\(Y\\) indicate that they are being estimated from data (they are statistics or estimates rather than parameters now!)\nNote that the error term is gone now: we know exactly what our estimate \\(\\hat Y\\) is, with no error.\n\nBut \\(\\hat Y\\) isn’t (usually) identical to \\(Y\\). The difference is our friend the residual:\n\n\\[\n\\begin{align*}\n\\mbox{residual} = \\hat\\varepsilon & = Y - \\hat Y \\\\\n& = \\mbox{observed reponse} - \\mbox{model response}\n\\end{align*}\n\\]\nAgain, the hats indicate that we are making estimates using data (hats basically always mean this in statistics). Those “beta hat” and “Y hat” values come from our fitted model. The \\(\\hat \\beta\\)s are called the coefficients (or, more precisely, the estimated coefficients) or the “beta hats”.\n\nYou will sometimes encounter slightly different notation for linear models. Two common things are:\n\nThe use of \\(b_0, b_1, \\dots, b_p\\) instead of \\(\\hat \\beta_0, \\hat \\beta_1, \\dots \\hat \\beta_p\\).\nDropping of hats. In particular, the hat is often left off of the response variable when the model is presented with the numerical coefficient estimates plugged in.\n\nAnd the models themselves go by different names: regression model, least squares model, least squares regression model, etc.\nDon’t let these little differences distract you. If you focus on the important concepts, you should be able to navigate the notation, whichever way someone does it.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#toy-dataset-pigeons",
    "href": "lm-fit.html#toy-dataset-pigeons",
    "title": "Fitting Linear Models in R",
    "section": "Toy Dataset: Pigeons",
    "text": "Toy Dataset: Pigeons\nFor the next few examples, we’ll consider a very small dataset on racing pigeons from Michigan. Their unirate scores measure their racing performance (smaller is better).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnirate\nOwnerName\nSex\nAge\n\n\n\n\n2.864\nKENNIER PALMIRA\nH\n1.9\n\n\n2.965\nTOM RICHARDS\nH\n1.5\n\n\n4.304\nKENNIER PALMIRA\nH\n2.3\n\n\n5.027\nTOM RICHARDS\nC\n2.2\n\n\n9.189\nTOM KUIPER\nC\n2.8",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#fitting-a-normal-distribution",
    "href": "lm-fit.html#fitting-a-normal-distribution",
    "title": "Fitting Linear Models in R",
    "section": "Fitting a normal distribution",
    "text": "Fitting a normal distribution\nLet’s try to reformulate a familiar exercise – fitting a normal distribution to a dataset – as a linear regression problem.\nWe might want to consider a model where the pigeon Unirate scores are \\(iid\\) (independent, identically distributed) samples from a \\(N(\\mu, \\sigma)\\) distribution.\n\n\nHow could we reformulate that model as a linear regression?\n\n Two 4 x 46 One 4 x 47 None\n\n\n\n\n\nClick for explanations of solution above.\n\nWhat predictor(s) does the unirate score depend on in this model? Well…none, in fact!\n\n\nA no-predictor model\nIt can make sense to think of the residuals \\(\\mathbf{\\epsilon}\\) of a linear regression having a \\(N(0, \\sigma)\\) distribution – they are normally distributed, and the model is right on average, but not spot on for every data observation. This gives us the “intercept-only” regression model:\n\\[ \\mathbf{Y} = \\mu \\mathbf{1} + \\epsilon,\\]\n\\[\\epsilon \\stackrel{iid}{\\sim} N(0, \\sigma)\\]\nHere \\(\\mathbf{X}\\) is just a column vector of ones, since there are no predictors, and the intercept \\(\\beta_0\\) is \\(\\mu\\), the mean Unirate score.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#one-predictor",
    "href": "lm-fit.html#one-predictor",
    "title": "Fitting Linear Models in R",
    "section": "One Predictor",
    "text": "One Predictor\nOf course, we can also fit models with one or more predictor variables.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#finding-the-betas",
    "href": "lm-fit.html#finding-the-betas",
    "title": "Fitting Linear Models in R",
    "section": "Finding the Betas",
    "text": "Finding the Betas\nOK, but how do we go from a model plan and dataset to an actual fitted model? In other words, how do we estimate the \\(\\beta\\)s and find the best-fitting regression model?\nTo find the “best-fitting” regression model for a dataset, we need to first define a metric to measure how “well” a line fits, and then find the \\(\\beta\\)s (intercept and slope(s)) that maximize it. (Actually, we’ll come up with a way to measure the mismatch between a line and the data, and find the \\(\\beta\\)s that minimize it - but it’s the same idea.)\nIn other words, our goal at the moment is to figure out how to estimate the \\(\\beta\\)s.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#least-squares-visually",
    "href": "lm-fit.html#least-squares-visually",
    "title": "Fitting Linear Models in R",
    "section": "Least Squares (visually)",
    "text": "Least Squares (visually)\nOne idea is to choose the “best-fit” line as the one that minimizes the sum of squared residuals.\nThis method is often called Least Squares Estimation (or Ordinary Least Squares).\nFirst, check out Ordinary Least Squares Estimation (explained visually). Be sure to take advantage of the interactive elements to play around!)",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#least-squares-practically",
    "href": "lm-fit.html#least-squares-practically",
    "title": "Fitting Linear Models in R",
    "section": "Least Squares (practically)",
    "text": "Least Squares (practically)\nNext, try your hand at off-the-cuff least squares estimation. Visit the PhET interactive simulator and:\n\nPick an example from the central upper pull-down menu (or create your own dataset) and:\n\nChoose your best-guess slope and intercept (menu on the right)\nCompare your result with the best-fit line (menu on the left). How close were you? Why/how do you think you went wrong?\nView the residuals, and the squared residuals, for the best-fit line.\nVerify that you understand exactly what the residuals and the SSE = RSE = sum of squared residuals are measuring. In what sense does the minimal-SSE line come “closest” to the data points?\n\nRepeat the exercise for at least one more example.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#least-squares-explained",
    "href": "lm-fit.html#least-squares-explained",
    "title": "Fitting Linear Models in R",
    "section": "Least Squares (explained)",
    "text": "Least Squares (explained)\nOptionally, if you would like one more presentation of the idea of least squares fitting, watch the (slightly boring but very clear) StatQuest video explanation:\n\n\n(You can also watch directly on YouTube if you prefer.)",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#in-r-lm",
    "href": "lm-fit.html#in-r-lm",
    "title": "Fitting Linear Models in R",
    "section": "In R: lm()",
    "text": "In R: lm()\nSo, now we understand the principle of least squares estimation. But we certainly won’t employ it via guess-and-check to actually fit models to real data!\nIn fact, either via calculus or linear algebra, it’s possible to obtain formulas for the slope and intercept that minimize the SSE for a given dataset. (And, of course, software knows how to use them.)\nThe function we’ll use to fit a linear regression model in R is lm().\nThe first input to lm() (and basically all other R functions that fit regression models) is a model formula of the form:\n \n\n\nlm ( &yy ~ &xx , data = mydata )\n\n\n \nHow do we fill in the empty boxes?\n\nModel Formula: Left Hand Side\nThe left hand side of the formula is simplest: we just need to provide the name of the response variable that we want to model.\n \n\n\nlm (  Y  ~ &xx , data = mydata )\n\n\n \nFor example, if we use dataset MI_lead and our response variable is ELL2012, the skeleton of our formula would look like:\n\nmy_model &lt;- lm( ELL2012 ~ _______, data = MI_lead)\n\nDataset note: The MI_lead dataset stores CDC-provided data on Michigan kids’ blood lead levels, by county, from 2005 and 2012. The dataset is at https://sldr.netlify.app/data/MI_lead.csv and also provides:\n\nCounty name\nProportion kids with elevated blood lead levels, ELL in years 2005 and 2012\nthe Difference in proportion kids with high lead levels between the two years (2012-2005)\nThe proportion of houses in the county that were built before 1950 (and thus more likely have lead paint), PropPre1950\nWhich Peninsula of MI the county is in\n\n\n\nModel Formula: Right Hand Side\nOn the other side of the formula (after the tilde symbol \\(\\sim\\)), we need to specify the name of the predictor variable(s).\nWhile this initial example is a simple linear regression (“simple” means just one predictor), it’s possible to have multiple ones, separated by +.\n\nmy_model &lt;- lm(ELL2012 ~ ELL2005 + Peninsula, \n               data = MI_lead)\nsummary(my_model)\n\n\nCall:\nlm(formula = ELL2012 ~ ELL2005 + Peninsula, data = MI_lead)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0048244 -0.0013235 -0.0006547  0.0012533  0.0072038 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.0012767  0.0003786   3.372  0.00116 ** \nELL2005         0.1576778  0.0311305   5.065 2.67e-06 ***\nPeninsulaUpper -0.0006220  0.0008028  -0.775  0.44083    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.002407 on 78 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2758,    Adjusted R-squared:  0.2573 \nF-statistic: 14.86 on 2 and 78 DF,  p-value: 3.417e-06\n\n\n\n\nPractice\nYour turn: fit a few linear regression models of your own. You can use the MI_lead data, or use one of the other suggested datasets (already read in for you here):\n\nelephantSurvey\ngapminder_clean\nHaDiveParameters\n\nEach time, consider:\n\nHow do you choose a response and a predictor variable?\n\nUsually the response is the main variable of interest, that you would like to predict or understand.\nThe predictor might cause, or be associated with, changes in the response; it might be easier to measure. Or, we might want to test whether it is associated with changes in the response or not.\n\nSee if you can express a scientific question which can be answered by your linear regression model.\n\nFor example, a model with formula ELL2012 ~ ELL2005 would answer, “Does the proportion of kids with elevated lead levels in a county in 2005 predict the proportion in 2012? (Do the same locations have high/low levels in the two years?)”\n\nDon’t forget to consider your understanding of causal relationships between variables of interest in planning your model - consdier drawing a causal diagram to help you decide which predictors should be included.\n\n\n\n\n\n\n\n\n\n\n\nIntercepts\nA (potentially non-zero) intercept is always included in all lm() models, by default.\nIf you wish, you can specifically tell R to include it (which doesn’t change the default behavior, but just makes it explicit). You do this by using a right-hand-side formula like 1 + predictor:\n\nmy_model &lt;- lm(ELL2012 ~ 1 + ELL2005, data = MI_lead)\n\n\n\nOmitting the Intercept\nYou can omit estimation of an intercept by replacing that 1 with a 0 or a -1. This will force the intercept to be 0 (line goes through the origin).\nThe 0 makes sense to me, because it’s like you’re forcing the first column of the model matrix to contain zeros instead of ones, multiplying the intercept by 0 to force it to be 0.\n(I’m not sure of the logic of the -1.)\nFor example,\n\nmy_model &lt;- lm(ELL2012 ~ 0 + ELL2005 + Peninsula,\n               data = MI_lead)\n\nBut usually, we want to estimate an intercept and we don’t need this code.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#interpreting-summarylm...",
    "href": "lm-fit.html#interpreting-summarylm...",
    "title": "Fitting Linear Models in R",
    "section": "Interpreting summary(lm(...))",
    "text": "Interpreting summary(lm(...))\nOnce you’ve fitted an lm() in R, how can you view and interpret the results? You may have already noticed the use above of the R function summary(). Let’s consider in more detail…\n\n\n(You can also watch directly on YouTube if you prefer.)",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#model-equation-practice",
    "href": "lm-fit.html#model-equation-practice",
    "title": "Fitting Linear Models in R",
    "section": "Model Equation Practice",
    "text": "Model Equation Practice\nYou should be able to use lm() to fit a linear model, and then use summary() output to fill in numerical parameter estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\), \\(\\hat{\\beta}_3\\), \\(\\hat{\\beta}_n\\), and \\(\\hat{\\sigma}\\) in the regression equation:\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon, \\epsilon \\sim \\text{Norm}(0, \\sigma)\\]\n\nAn example model\nTo practice, let’s fit a simple linear regression model in R. For data, let’s consider a dataset containing scores of 542 people who chose to take an online nerdiness quiz. Higher scores mean more nerdiness. The participants also provided their ages. Variables in the data include score and age. Does someone’s age predict their nerdiness score?\nPlot the data and use lm() to fit a simple linear regression model (just one predictor) to explore this question. The code to read in the dataset is provided for you.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHint 1\n\n\n\n\n\nnerds &lt;- read.csv('https://sldr.netlify.app/data/nerds.csv')\ngf_point(_____ ~ _____, data = ______)\n\n\n\n\n\n\n\n\n\n\n\nTipSolution:\n\n\n\n\n\nnerds &lt;- read.csv('https://sldr.netlify.app/data/nerds.csv')\ngf_point(score ~ age, data = nerds)\nnerd_model &lt;- lm(score ~ age, data = nerds)\nsummary(nerd_model)\n\n\n\n\n\n#| label: nerd-eqn-quiz\nopts1 &lt;- sample(c(answer = “95.8”, “-0.061”, “0.22”, “15.81”, ‘0.00092’))\ncat(“What is beta0 for the model you just fitted?”, longmcq(opts1))\nopts2 &lt;- sample(c(answer = “-0.061”, “95.8”, “0.22”, “15.81”, ‘0.00092’))\ncat(“What is beta1 for the model you just fitted?”, longmcq(opts2))\nopts3 &lt;- sample(c(answer = “15.81”, “95.8”, “0.22”, “-0.061”, ‘0.00092’))\ncat(“What is sigma, the standard deviation of the residuals, for the model you just fitted?”, longmcq(opts3)) ` ``",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "lm-fit.html#linear-regression-conditions",
    "href": "lm-fit.html#linear-regression-conditions",
    "title": "Fitting Linear Models in R",
    "section": "Linear Regression Conditions",
    "text": "Linear Regression Conditions\nWhat’s next? Before we interpret the results of a model, we have to be confident that we trust the model is appropriate enough for the data, and fits well enough, to produce results that will be reliable.\nSo, next module, we’ll learn to do model assessment (checking that conditions are met to verify that a model is appropriate for a particular dataset).\nWe will also go into much more detail about how to interpret our fitted model!",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Fitting and Equation"
    ]
  },
  {
    "objectID": "r-projects.html",
    "href": "r-projects.html",
    "title": "RStudio Projects",
    "section": "",
    "text": "It is a good idea to do your work in RStudio using Projects. A project if basically just a folder (and any files and sub-folders within it) that you have told RStudio are related.\nYou can use the Projects menu to quickly switch among your different projects. You can create different projects for different courses, different (types of) assignments, for your group project, etc. And you can have some settings apply on a per-project basis.\nOne advantage of working in RStudio Server is the ability to easily share projects with other users.\nThis guide will walk you through the steps to\n\ncreate a new project,\nshare it with other users, and\ncollaborate on documents within the project. (This is a bit like working in Google Docs, where multiple users can edit the same document simultaneously.)",
    "crumbs": [
      "Home",
      "R Basics",
      "RStudio Projects"
    ]
  },
  {
    "objectID": "r-projects.html#overview",
    "href": "r-projects.html#overview",
    "title": "RStudio Projects",
    "section": "",
    "text": "It is a good idea to do your work in RStudio using Projects. A project if basically just a folder (and any files and sub-folders within it) that you have told RStudio are related.\nYou can use the Projects menu to quickly switch among your different projects. You can create different projects for different courses, different (types of) assignments, for your group project, etc. And you can have some settings apply on a per-project basis.\nOne advantage of working in RStudio Server is the ability to easily share projects with other users.\nThis guide will walk you through the steps to\n\ncreate a new project,\nshare it with other users, and\ncollaborate on documents within the project. (This is a bit like working in Google Docs, where multiple users can edit the same document simultaneously.)",
    "crumbs": [
      "Home",
      "R Basics",
      "RStudio Projects"
    ]
  },
  {
    "objectID": "r-projects.html#create-a-new-project",
    "href": "r-projects.html#create-a-new-project",
    "title": "RStudio Projects",
    "section": "Create a New Project",
    "text": "Create a New Project\nIn order to share a project, you must first create one. Click on the Project Menu in the upper right of RStudio (it may say “Project: (None)” if you don’t have any projects yet) and select New Project.\n\n\n\nA New Project Wizard window will open prompting you to select the type of project. Select the New Directory option and then select New Project.\nGive your project a descriptive name.\nClick the Browse button to select a folder where you want to save this project.\n\n\n\n\n\n\nWarningShared projects must live in a special place\n\n\n\nIf you plan to share your project with others, you must save it in a special place:\n\n/rprojects/&lt;your_user_name&gt;.\n\nThis folder lives outside your home directory, so a special process is involved in selecting the location.\nClick Browse and then the three little dots (…) in the upper right. Now enter /rprojects/&lt;your_user_name&gt; in the entry box.\n\n\n\n\n\nOnce you have chosen the directoty name and the directory to create it in (“Create project as a subdirectory of”), you can choose Create Project.\n\n\n\nYour project should open automatically.\nThe project menu in the upper right of the page should now be labeled with the name of your project and the folder containing your project should be displayed in the Files tab.\nFrom now on, any document that you add to the project folder will be part of the project. Add a Quarto file to your project by clicking on File &gt; New File &gt; Quarto",
    "crumbs": [
      "Home",
      "R Basics",
      "RStudio Projects"
    ]
  },
  {
    "objectID": "r-projects.html#shared-projects-server-only",
    "href": "r-projects.html#shared-projects-server-only",
    "title": "RStudio Projects",
    "section": "Shared Projects (server only)",
    "text": "Shared Projects (server only)\n\nSharing a Project\nShared projects must live in /rprojects/&lt;your_user_name&gt;. See @warning-rprojects for details.\nProject Menu in the upper right of RStudio and select Share Project.\n\n\n\nEnter the username of the user that you would like to share the project with, and then click the Add button. Click on the OK button when you are finished adding collaborators.\n\n\n\n\n\n\n\n\n\nNoteEach shared project has a primary “owner”.\n\n\n\nIf you are working as a team, pick one person to create and share the project. That person will be the “owner” of the project.\n\n\n\n\nOpen a Shared Project\nTo open a project that has been shared with you click on the Project Menu in the upper right of RStudio and select Open Project. Click on the Shared with Me tab at the top of the window to display the projects that are shared with you.\n\n\n\nSelect the desired project and click Open Project.\n\n\nCollaborating\nMultiple users can simultaneously work on any document in a shared project. If a user signs on and works on the document while you are working on the document a colored icon will appear at the top of the page. Hover on the icon to see the user’s info.",
    "crumbs": [
      "Home",
      "R Basics",
      "RStudio Projects"
    ]
  },
  {
    "objectID": "quarto-math.html",
    "href": "quarto-math.html",
    "title": "Mathematical Notation in Quarto",
    "section": "",
    "text": "You might be wondering…\n\n\nHow can I include Greek letters and other symbols in the text part of my Quarto (or RMarkdown) document?\n\n\nBasically, you enclose the name of the symbol you want with $\\ …$\n(if you use LaTeX, this will be very familiar):\n\n\n\nType this in qmd:\nTo get this when rendered:\n\n\n\n\n$\\hat{p}$\n\\(\\hat{p}\\)\n\n\n$\\bar{x}$\n\\(\\bar{x}\\)\n\n\n$\\alpha$\n\\(\\alpha\\)\n\n\n$\\beta$\n\\(\\beta\\)\n\n\n$\\gamma$\n\\(\\gamma\\)\n\n\n$\\Gamma$\n\\(\\Gamma\\)\n\n\n$\\mu$\n\\(\\mu\\)\n\n\n$\\sigma$\n\\(\\sigma\\)\n\n\n$\\sigma^2$\n\\(\\sigma^2\\)\n\n\n$\\rho$\n\\(\\rho\\)\n\n\n$\\epsilon$\n\\(\\epsilon\\)\n\n\n$\\sim$\n\\(\\sim\\)\n\n\n$\\mu_D$\n\\(\\mu_D\\)\n\n\n$\\mu_{longsubscript}$\n\\(\\mu_{longsubscript}\\)\n\n\n$\\hat{p}_{longsubscript}$\n\\(\\hat{p}_{longsubscript}\\)\n\n\n$\\mu\\neq 0$\n\\(\\mu \\neq 0\\)\n\n\n$\\mu\\geq 5$\n\\(\\mu \\geq 5\\)\n\n\n$\\mu\\leq 1$\n\\(\\mu \\leq 1\\)\n\n\n$\\cup$\n\\(\\cup\\)\n\n\n$\\cap$\n\\(\\cap\\)\n\n\n$\\vert$\n\\(\\vert\\)\n\n\n$\\sim$\n\\(\\sim\\)\n\n\n$\\frac{numerator}{denominator}$\n\\(\\frac{numerator}{denominator}\\)\n\n\n\nFor other Greek letters, just spell out the name of the letter that you want (following the models above). If you want a capital Greek letter, capitalize the first letter of its name when you write it out (e.g. Sigma instead of sigma).\nNote: Avoid spaces before the final $ or after the initial $.",
    "crumbs": [
      "Home",
      "Quarto",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "quarto-math.html#greek-letters-common-symbols-subscripts-and-superscripts",
    "href": "quarto-math.html#greek-letters-common-symbols-subscripts-and-superscripts",
    "title": "Mathematical Notation in Quarto",
    "section": "",
    "text": "You might be wondering…\n\n\nHow can I include Greek letters and other symbols in the text part of my Quarto (or RMarkdown) document?\n\n\nBasically, you enclose the name of the symbol you want with $\\ …$\n(if you use LaTeX, this will be very familiar):\n\n\n\nType this in qmd:\nTo get this when rendered:\n\n\n\n\n$\\hat{p}$\n\\(\\hat{p}\\)\n\n\n$\\bar{x}$\n\\(\\bar{x}\\)\n\n\n$\\alpha$\n\\(\\alpha\\)\n\n\n$\\beta$\n\\(\\beta\\)\n\n\n$\\gamma$\n\\(\\gamma\\)\n\n\n$\\Gamma$\n\\(\\Gamma\\)\n\n\n$\\mu$\n\\(\\mu\\)\n\n\n$\\sigma$\n\\(\\sigma\\)\n\n\n$\\sigma^2$\n\\(\\sigma^2\\)\n\n\n$\\rho$\n\\(\\rho\\)\n\n\n$\\epsilon$\n\\(\\epsilon\\)\n\n\n$\\sim$\n\\(\\sim\\)\n\n\n$\\mu_D$\n\\(\\mu_D\\)\n\n\n$\\mu_{longsubscript}$\n\\(\\mu_{longsubscript}\\)\n\n\n$\\hat{p}_{longsubscript}$\n\\(\\hat{p}_{longsubscript}\\)\n\n\n$\\mu\\neq 0$\n\\(\\mu \\neq 0\\)\n\n\n$\\mu\\geq 5$\n\\(\\mu \\geq 5\\)\n\n\n$\\mu\\leq 1$\n\\(\\mu \\leq 1\\)\n\n\n$\\cup$\n\\(\\cup\\)\n\n\n$\\cap$\n\\(\\cap\\)\n\n\n$\\vert$\n\\(\\vert\\)\n\n\n$\\sim$\n\\(\\sim\\)\n\n\n$\\frac{numerator}{denominator}$\n\\(\\frac{numerator}{denominator}\\)\n\n\n\nFor other Greek letters, just spell out the name of the letter that you want (following the models above). If you want a capital Greek letter, capitalize the first letter of its name when you write it out (e.g. Sigma instead of sigma).\nNote: Avoid spaces before the final $ or after the initial $.",
    "crumbs": [
      "Home",
      "Quarto",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "quarto-math.html#summations-and-products",
    "href": "quarto-math.html#summations-and-products",
    "title": "Mathematical Notation in Quarto",
    "section": "Summations and Products",
    "text": "Summations and Products\n\n\n\nType This:\nTo get this in your PDF:\n\n\n\n\n$\\sum_{i=1}^{n} x_i$\n\\(\\sum_{i=1}^{n} x_i\\)\n\n\n$\\prod_{i=1}^{n} f(i)}$\n\\(\\prod_{i=1}^{n} f(i)\\)\n\n\n\nThese will format as seen above if used in inline math mode (enclosed in single $s). If you put them in display math mode by using two $$ at the start and end instead of just one, then the result will be displayed centered on its own line and the limits of the summation/product will be above/below the \\(\\Sigma\\) or \\(\\Pi\\):\n\n\n\\[\\prod_{i=1}^{n} f(i)\\]",
    "crumbs": [
      "Home",
      "Quarto",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "quarto-math.html#long-equations",
    "href": "quarto-math.html#long-equations",
    "title": "Mathematical Notation in Quarto",
    "section": "Long equations",
    "text": "Long equations\nYou can use double $ to bracket equations you want to display on a line of their own. Inside can be multiple mathematical expressions. For example:\n$$y = \\beta_0 + \\beta_1x_1 + \\epsilon,$$\n$$\\epsilon \\sim N(0, \\sigma)$$\ngives\n\\[y = \\beta_0 + \\beta_1x_1 + \\epsilon,\\] \\[\\epsilon \\sim N(0, \\sigma)\\]\nNote: Avoid spaces before the final $ or after the initial $.",
    "crumbs": [
      "Home",
      "Quarto",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "r-install.html",
    "href": "r-install.html",
    "title": "Install R and RStudio",
    "section": "",
    "text": "One way to access R and RStudio is via an account at posit.cloud or on a posit Workbench server (like https://r.stem.calvin.edu). All we need to do is log in, with nothing to install or maintain.\nFor many, this cloud approach is a great way to use RStudio, and they have no reason to install a standalone copy of the software on a personal computer. If you are happy using a server, exit this tutorial now and continue happily using the server!\nIf you have concerns about your internet bandwidth, speed, usage limits, or firewalls, or if you want to be able to work on assignments for this class somewhere without internet access, or if you need to analyze really large datasets or fit complex models, you may want to install R and RStudio and work locally instead of on the server.\n\n\nThe benefits of downloading your own copy are that you can work offline and should not be subject to any (hopefully rare) server-related errors, freezing, etc.\nThe negatives of downloading your own copy are that you have to maintain it yourself, installing and updating packages and software.",
    "crumbs": [
      "Home",
      "R Basics",
      "Install R/RStudio"
    ]
  },
  {
    "objectID": "r-install.html#motivation",
    "href": "r-install.html#motivation",
    "title": "Install R and RStudio",
    "section": "",
    "text": "One way to access R and RStudio is via an account at posit.cloud or on a posit Workbench server (like https://r.stem.calvin.edu). All we need to do is log in, with nothing to install or maintain.\nFor many, this cloud approach is a great way to use RStudio, and they have no reason to install a standalone copy of the software on a personal computer. If you are happy using a server, exit this tutorial now and continue happily using the server!\nIf you have concerns about your internet bandwidth, speed, usage limits, or firewalls, or if you want to be able to work on assignments for this class somewhere without internet access, or if you need to analyze really large datasets or fit complex models, you may want to install R and RStudio and work locally instead of on the server.\n\n\nThe benefits of downloading your own copy are that you can work offline and should not be subject to any (hopefully rare) server-related errors, freezing, etc.\nThe negatives of downloading your own copy are that you have to maintain it yourself, installing and updating packages and software.",
    "crumbs": [
      "Home",
      "R Basics",
      "Install R/RStudio"
    ]
  },
  {
    "objectID": "r-install.html#goal",
    "href": "r-install.html#goal",
    "title": "Install R and RStudio",
    "section": "Goal",
    "text": "Goal\nThis document will guide you through the process of installing R, RStudio, and other necessary R packages on your own computer, if you choose to do so. Again, there is no course requirement to do this.\nThe process will have three stages, which work best in order:\n\nInstall R\nInstall RStudio\nInstall necessary R packages from within RStudio",
    "crumbs": [
      "Home",
      "R Basics",
      "Install R/RStudio"
    ]
  },
  {
    "objectID": "r-install.html#downloadinstall-1-of-2-r",
    "href": "r-install.html#downloadinstall-1-of-2-r",
    "title": "Install R and RStudio",
    "section": "Download/Install 1 of 2: R",
    "text": "Download/Install 1 of 2: R\nR downloads are available from https://cran.r-project.org/.\n\n\n\n\n\n\n\n\n\n\nSelect the download that matches your operating system and hardware (Mac OS, Windows, Linux, etc.)\nYou only need the “base” version.\nDownload the installer and run it. You may want to choose not to create shortcuts, since you will access R only through RStudio.\n\n\nMac with Homebrew\nWindows and Linux users: skip this section.\n\nIf working on Mac OS and already using Homebrew to manage software packages, you can skip the manual download above and just run:\n\n\nbrew install r\n\n\nIf you want to get Homebrew and install this way on a mac, there are detailed instructions online – scroll down to “Instructions for Mac Users”. Note that you don’t necessarily need OpenBLAS for this course (as recommended on the linked website); it does not really matter either way.",
    "crumbs": [
      "Home",
      "R Basics",
      "Install R/RStudio"
    ]
  },
  {
    "objectID": "r-install.html#downloadinstall-2-of-2-rstudio",
    "href": "r-install.html#downloadinstall-2-of-2-rstudio",
    "title": "Install R and RStudio",
    "section": "Download/Install 2 of 2: RStudio",
    "text": "Download/Install 2 of 2: RStudio\nOnce you have installed R, you next need to install RStudio.\n\nDownloads of RStudio are available at https://rstudio.com/products/rstudio/download/.\nYou should select the free version of RStudio Desktop.\nDownload and install the version that matches your operating system\n\n\n\n\n\n\n\n\n\n\n\nMac with Homebrew\nWindows and Linux users: you can’t use Homebrew.\n\nIf using a mac and Homebrew, you can alternatively install RStudio via:\n\n\nbrew cask install rstudio",
    "crumbs": [
      "Home",
      "R Basics",
      "Install R/RStudio"
    ]
  },
  {
    "objectID": "r-install.html#install-33-packages",
    "href": "r-install.html#install-33-packages",
    "title": "Install R and RStudio",
    "section": "Install 3/3: Packages",
    "text": "Install 3/3: Packages\nIn addition to base R and the RStudio IDE, we use a few add-on packages that you will need to install yourself.\n\nOpen RStudio\nIn your RStudio Console window, which is on the lower left by default, type (or copy and paste) the code below and click “Enter” to run it:\n\n\ninstall.packages(c('rmarkdown', # reproducible research documents\n                   'tidyverse', 'remotes', # graphics and data wrangling\n                   'pander', # formatting tables\n                   'glmmTMB', 'mgcv', # fitting regression models\n                   'car', 'ggeffects'#,  working with fitted models\n                   # optional additions:\n                   # 'mosaic', # formula-based summary stats and resampling\n                   # 'openintro', # datasets\n                   # 'shiny', 'plotly', 'gganimate', 'leaflet' # interactive graphics/maps\n                   )) \n# if desired, for function to ggplot ACFs:\n# remotes::install_github('stacyderuiter/s245')\n\n\nIn addition to the packages you listed specifically, a number of dependencies (other packages that the packages you requested require to work) will be installed.\nThe amount of time it takes will depend on your computer and internet connection speed, but as long as it finishes without any messages that literally say “Error: …”, it worked!.\nIf RStudio prompts you to update packages or install additional dependencies, it’s usually a good idea to do so.\nIf R asks you if you want to install a certain package “from source” blah, blah, “is newer…” usually you can answer yes (or no) and it will work either way.\nIf you get an error or have any questions, get in touch with your professor.\n\n\nTeX for PDF generation\nTo enable generation of PDF output from Rmarkdown documents, there is a little more code to run. (If you don’t know what this means yet, you will soon - and you do probably need to be able to do it.)\nThis one has two steps: installing the package, and then using the package to install the PDF-generation utility.\nIf you already have TeX/LaTeX/MikTeX installed on your computer, you can probably skip this installation (but it won’t hurt).\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX\n\n\n\nPoint-and-Click Option\nIf you would like to install the packages interactively instead of on the command line (as already shown above), you can click the Packages tab on the lower right in RStudio, then click Install at the top of the tab. Enter the names of the packages you want to install in the middle “Packages” blank, and leave the rest of the default options, then click “Install”.",
    "crumbs": [
      "Home",
      "R Basics",
      "Install R/RStudio"
    ]
  },
  {
    "objectID": "r-install.html#you-did-it",
    "href": "r-install.html#you-did-it",
    "title": "Install R and RStudio",
    "section": "You did it!",
    "text": "You did it!\nIf you complete all three steps above, you should have a working version of RStudio on your machine. To use it, just open RStudio; it should look nearly identical to the RStudio Server version you have been using online.\nYou don’t ever have to open or access R directly; RStudio does it all for you.\nIn case of any errors or problems, contact me (stacy.deruiter at calvin.edu) anytime and I’ll do my best to help.\n(Don’t contact school help desks; they don’t support this software).\n\n\n\n\n\n\nNotePositron\n\n\n\nPosit has a newer IDE called Positron. It is very similar to VS Code, with some extra features to simplify data analysis. Especially if you are already familiar with VS Code, you might consider using Positron instead of RStudio. Give them both a try and see which you prefer.\nIn the next year or so, Posit expects to have cloud versions of Positron available. For now, it is only available on your local machine.",
    "crumbs": [
      "Home",
      "R Basics",
      "Install R/RStudio"
    ]
  }
]